{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "varying-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "improved-assets",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.read_csv(\"../data/cleaned.csv\")\n",
    "images = pd.read_csv(\"../data/images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ranging-insight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>bigram</th>\n",
       "      <th>tweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>new view love art presentation perspective lig...</td>\n",
       "      <td>newview viewlove loveart artpresentation prese...</td>\n",
       "      <td>New view. Love the art. Presentation. Perspect...</td>\n",
       "      <td>2021-03-17 16:24:35 EDT</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtT4E7UUAAgt5w.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>st paddy’s day edition great wall color maps a...</td>\n",
       "      <td>stpaddy paddy’ ’s sday dayedition editiongreat...</td>\n",
       "      <td>Room Rater St Paddy’s Day Edition. Great wall ...</td>\n",
       "      <td>2021-03-17 15:25:51 EDT</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtGbzTUYAAxDiu.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>doors dublin dingus-the stuff dreams made nice...</td>\n",
       "      <td>doorsdublin dublindingus-the dingus-thestuff s...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "      <td>2021-03-17 15:19:53 EDT</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>good tight set up love blue art flag widen sho...</td>\n",
       "      <td>goodtight tightset setup uplove loveblue bluea...</td>\n",
       "      <td>Good tight set up. Love the blue. Art. Flag. W...</td>\n",
       "      <td>2021-03-17 13:46:47 EDT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>https://pbs.twimg.com/media/EwsvwlOU8AQ5lOY.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>dark never escape stank wrong about everything...</td>\n",
       "      <td>darknever neverescape escapestank stankwrong w...</td>\n",
       "      <td>Dark. Will never escape the stank. Wrong about...</td>\n",
       "      <td>2021-03-17 13:11:17 EDT</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>846</td>\n",
       "      <td>https://pbs.twimg.com/media/Ewsnoq5U8AMxxAg.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "0       9  new view love art presentation perspective lig...   \n",
       "1       9  st paddy’s day edition great wall color maps a...   \n",
       "2      10  doors dublin dingus-the stuff dreams made nice...   \n",
       "3       9  good tight set up love blue art flag widen sho...   \n",
       "4       0  dark never escape stank wrong about everything...   \n",
       "\n",
       "                                              bigram  \\\n",
       "0  newview viewlove loveart artpresentation prese...   \n",
       "1  stpaddy paddy’ ’s sday dayedition editiongreat...   \n",
       "2  doorsdublin dublindingus-the dingus-thestuff s...   \n",
       "3  goodtight tightset setup uplove loveblue bluea...   \n",
       "4  darknever neverescape escapestank stankwrong w...   \n",
       "\n",
       "                                               tweet               created_at  \\\n",
       "0  New view. Love the art. Presentation. Perspect...  2021-03-17 16:24:35 EDT   \n",
       "1  Room Rater St Paddy’s Day Edition. Great wall ...  2021-03-17 15:25:51 EDT   \n",
       "2  Doors of Dublin. The Dingus-the stuff that dre...  2021-03-17 15:19:53 EDT   \n",
       "3  Good tight set up. Love the blue. Art. Flag. W...  2021-03-17 13:46:47 EDT   \n",
       "4  Dark. Will never escape the stank. Wrong about...  2021-03-17 13:11:17 EDT   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  \\\n",
       "0              3               2           62   \n",
       "1              2               2           57   \n",
       "2              7               4          123   \n",
       "3              1               1           60   \n",
       "4             35              38          846   \n",
       "\n",
       "                                           img_url  \n",
       "0  https://pbs.twimg.com/media/EwtT4E7UUAAgt5w.jpg  \n",
       "1  https://pbs.twimg.com/media/EwtGbzTUYAAxDiu.jpg  \n",
       "2  https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg  \n",
       "3  https://pbs.twimg.com/media/EwsvwlOU8AQ5lOY.jpg  \n",
       "4  https://pbs.twimg.com/media/Ewsnoq5U8AMxxAg.jpg  "
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "traditional-national",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'new view love art presentation perspective light/lighting widen shot slightly @davidjollyfl'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.iloc[0][\"cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "brave-modern",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book club love art lighting flowers arkansas traveler @iamsophianelson'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.iloc[7][\"cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "revolutionary-physiology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>bigram</th>\n",
       "      <th>tweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>doors dublin dingus-the stuff dreams made nice...</td>\n",
       "      <td>doorsdublin dublindingus-the dingus-thestuff s...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "      <td>2021-03-17 15:19:53 EDT</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>st paddy’s day update @philiprucker green tie</td>\n",
       "      <td>stpaddy paddy’ ’s sday dayupdate update@ @phil...</td>\n",
       "      <td>Room Rater St Paddy’s Day Update. @PhilipRucke...</td>\n",
       "      <td>2021-03-17 12:34:31 EDT</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>620</td>\n",
       "      <td>https://pbs.twimg.com/media/EwsfN-yVoAAcVk8.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>book club love art lighting flowers arkansas t...</td>\n",
       "      <td>bookclub clublove loveart artlighting lighting...</td>\n",
       "      <td>Room Rater Book Club. Love the art. Lighting. ...</td>\n",
       "      <td>2021-03-17 11:49:52 EDT</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "      <td>https://pbs.twimg.com/media/EwsVABRVgAEJGvE.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "2      10  doors dublin dingus-the stuff dreams made nice...   \n",
       "5      10      st paddy’s day update @philiprucker green tie   \n",
       "7      10  book club love art lighting flowers arkansas t...   \n",
       "\n",
       "                                              bigram  \\\n",
       "2  doorsdublin dublindingus-the dingus-thestuff s...   \n",
       "5  stpaddy paddy’ ’s sday dayupdate update@ @phil...   \n",
       "7  bookclub clublove loveart artlighting lighting...   \n",
       "\n",
       "                                               tweet               created_at  \\\n",
       "2  Doors of Dublin. The Dingus-the stuff that dre...  2021-03-17 15:19:53 EDT   \n",
       "5  Room Rater St Paddy’s Day Update. @PhilipRucke...  2021-03-17 12:34:31 EDT   \n",
       "7  Room Rater Book Club. Love the art. Lighting. ...  2021-03-17 11:49:52 EDT   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  \\\n",
       "2              7               4          123   \n",
       "5             10              12          620   \n",
       "7              1               4           87   \n",
       "\n",
       "                                           img_url  \n",
       "2  https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg  \n",
       "5  https://pbs.twimg.com/media/EwsfN-yVoAAcVk8.jpg  \n",
       "7  https://pbs.twimg.com/media/EwsVABRVgAEJGvE.jpg  "
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowest rating\n",
    "low_rating = cleaned[cleaned['rating'] == 0]\n",
    "low_rating.head(3) # yields row 4, 20, 28\n",
    "\n",
    "# semi low rating\n",
    "semi_low_rating = cleaned[cleaned['rating'] == 3]\n",
    "semi_low_rating.head(3) # yields row 75, 135, 153\n",
    "\n",
    "# middle rating\n",
    "middle_rating = cleaned[cleaned['rating'] == 5]\n",
    "middle_rating.head(3) # yields rows 169, 240, 245\n",
    "\n",
    "#semi middle rating\n",
    "semi_middle_rating = cleaned[cleaned['rating'] == 7]\n",
    "semi_middle_rating.head(3) # yields rows 37, 45, 69\n",
    "\n",
    "# semi high rating\n",
    "semi_high_rating = cleaned[cleaned['rating'] == 8]\n",
    "semi_high_rating.head(3) # yields rows 8, 9, 17\n",
    "\n",
    "# highest rating\n",
    "highest_rating = cleaned[cleaned['rating'] == 10]\n",
    "highest_rating.head(3) # yields rows 2, 5, 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "comic-india",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12619"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "trained-tactics",
   "metadata": {},
   "source": [
    "## **ADDITIONAL PREPROCESSING:**\n",
    "    * Removing user tags (starting with @)\n",
    "    * Stemming each cleaned word in tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "together-isolation",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = cleaned[['rating','cleaned']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "encouraging-salon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>new view love art presentation perspective lig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>st paddy’s day edition great wall color maps a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>doors dublin dingus-the stuff dreams made nice...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>good tight set up love blue art flag widen sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>dark never escape stank wrong about everything...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>st paddy’s day update @philiprucker green tie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>sun set ups tough raise camera late christmas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>book club love art lighting flowers arkansas t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ducks packers big plant well composed sports s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>well composed set up good spacing lower camera...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned\n",
       "0       9  new view love art presentation perspective lig...\n",
       "1       9  st paddy’s day edition great wall color maps a...\n",
       "2      10  doors dublin dingus-the stuff dreams made nice...\n",
       "3       9  good tight set up love blue art flag widen sho...\n",
       "4       0  dark never escape stank wrong about everything...\n",
       "5      10      st paddy’s day update @philiprucker green tie\n",
       "6       6  sun set ups tough raise camera late christmas ...\n",
       "7      10  book club love art lighting flowers arkansas t...\n",
       "8       8  ducks packers big plant well composed sports s...\n",
       "9       8  well composed set up good spacing lower camera..."
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "close-import",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(tweet):\n",
    "    if pd.isnull(tweet):\n",
    "        return []\n",
    "    #print(tweet)\n",
    "    tokens = tweet.split(' ')\n",
    "    tokens = list(filter(lambda x: '@' not in x, tokens))\n",
    "    return tokens\n",
    "\n",
    "def stemWords(tokens):\n",
    "    result = []\n",
    "    if tokens is None:\n",
    "        return tokens\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            stem = PorterStemmer()\n",
    "            stemmed_word = stem.stem(token, 0, len(token) - 1)\n",
    "            result.append(stemmed_word)\n",
    "        else: \n",
    "            result.append(token)\n",
    "\n",
    "    return result\n",
    "\n",
    "def split(tweet):\n",
    "    if pd.isnull(tweet):\n",
    "        return []\n",
    "    tokens = tweet.split(' ')\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "roman-platform",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "preprocessed['tags_removed'] = preprocessed['cleaned'].apply(lambda x : remove_tags(x))\n",
    "preprocessed['stemmed'] = preprocessed['tags_removed'].apply(lambda x : stemWords(x))\n",
    "preprocessed['cleaned'] = preprocessed['cleaned'].apply(lambda x : split(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "resistant-links",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tags_removed</th>\n",
       "      <th>stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>[new, view, love, art, present, perspect, ligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>[st, paddy’s, dai, edit, great, wall, color, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>[door, dublin, dingus-the, stuff, dream, made,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>[dark, never, escap, stank, wrong, about, ever...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12614</th>\n",
       "      <td>8</td>\n",
       "      <td>[blue, works, not, just, books, plus, good, us...</td>\n",
       "      <td>[blue, works, not, just, books, plus, good, us...</td>\n",
       "      <td>[blue, work, not, just, book, plu, good, us, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615</th>\n",
       "      <td>2</td>\n",
       "      <td>[“i’ll, just, put, sweatpants”, skype, rooms]</td>\n",
       "      <td>[“i’ll, just, put, sweatpants”, skype, rooms]</td>\n",
       "      <td>[“i’ll, just, put, sweatpants”, skype, room]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12616</th>\n",
       "      <td>3</td>\n",
       "      <td>[books, too, dark, little, way, personal, style]</td>\n",
       "      <td>[books, too, dark, little, way, personal, style]</td>\n",
       "      <td>[book, too, dark, littl, wai, person, style]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12617</th>\n",
       "      <td>4</td>\n",
       "      <td>[books, always, must, little, too, obvious, su...</td>\n",
       "      <td>[books, always, must, little, too, obvious, su...</td>\n",
       "      <td>[book, alwai, must, littl, too, obviou, such, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12618</th>\n",
       "      <td>2</td>\n",
       "      <td>[she’s, not, even, trying]</td>\n",
       "      <td>[she’s, not, even, trying]</td>\n",
       "      <td>[she’s, not, even, try]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12619 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                            cleaned  \\\n",
       "0           9  [new, view, love, art, presentation, perspecti...   \n",
       "1           9  [st, paddy’s, day, edition, great, wall, color...   \n",
       "2          10  [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3           9  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4           0  [dark, never, escape, stank, wrong, about, eve...   \n",
       "...       ...                                                ...   \n",
       "12614       8  [blue, works, not, just, books, plus, good, us...   \n",
       "12615       2      [“i’ll, just, put, sweatpants”, skype, rooms]   \n",
       "12616       3   [books, too, dark, little, way, personal, style]   \n",
       "12617       4  [books, always, must, little, too, obvious, su...   \n",
       "12618       2                         [she’s, not, even, trying]   \n",
       "\n",
       "                                            tags_removed  \\\n",
       "0      [new, view, love, art, presentation, perspecti...   \n",
       "1      [st, paddy’s, day, edition, great, wall, color...   \n",
       "2      [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3      [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4      [dark, never, escape, stank, wrong, about, eve...   \n",
       "...                                                  ...   \n",
       "12614  [blue, works, not, just, books, plus, good, us...   \n",
       "12615      [“i’ll, just, put, sweatpants”, skype, rooms]   \n",
       "12616   [books, too, dark, little, way, personal, style]   \n",
       "12617  [books, always, must, little, too, obvious, su...   \n",
       "12618                         [she’s, not, even, trying]   \n",
       "\n",
       "                                                 stemmed  \n",
       "0      [new, view, love, art, present, perspect, ligh...  \n",
       "1      [st, paddy’s, dai, edit, great, wall, color, m...  \n",
       "2      [door, dublin, dingus-the, stuff, dream, made,...  \n",
       "3      [good, tight, set, up, love, blue, art, flag, ...  \n",
       "4      [dark, never, escap, stank, wrong, about, ever...  \n",
       "...                                                  ...  \n",
       "12614  [blue, work, not, just, book, plu, good, us, s...  \n",
       "12615       [“i’ll, just, put, sweatpants”, skype, room]  \n",
       "12616       [book, too, dark, littl, wai, person, style]  \n",
       "12617  [book, alwai, must, littl, too, obviou, such, ...  \n",
       "12618                            [she’s, not, even, try]  \n",
       "\n",
       "[12619 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "institutional-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = list(chain(*preprocessed['stopwords']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "flexible-biology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140437"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-stationery",
   "metadata": {},
   "source": [
    "# **Utilizing LDA Topic Modeling**\n",
    "\n",
    "**Only LDA Topic Model:** \n",
    "   1. Fits data into LDA to get topic models, topic model is trained\n",
    "   2. Topic model is applied back to training reviews + each review gets topic\n",
    "      distribution/probabilities\n",
    "   3. Topic probabilities are used as features for rating prediction + fitted\n",
    "      into linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ethical-system",
   "metadata": {},
   "source": [
    "## Fitting Only LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "respective-reproduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessed has columns: cleaned, tags_removed, stemmed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-explosion",
   "metadata": {},
   "source": [
    "### Creating a Bag of Words for Cleaned + Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "french-daughter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bag of words\n",
    "\n",
    "# cleaned column\n",
    "token_dict_cleaned = Dictionary(preprocessed.cleaned)\n",
    "bag_of_words_cleaned = [token_dict_cleaned.doc2bow(tweet) for tweet in preprocessed['cleaned']]\n",
    "\n",
    "# stemmed\n",
    "token_dict_stemmed = Dictionary(preprocessed.stemmed)\n",
    "bag_of_words_stemmed = [token_dict_stemmed.doc2bow(tweet) for tweet in preprocessed['stemmed']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-connection",
   "metadata": {},
   "source": [
    "### Fitting Cleaned + Stemmed data into LDA model with 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "external-income",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.016*\"it’s\" + 0.013*\"not\" + 0.012*\"better\" + 0.010*\"like\" + 0.010*\"go\" + 0.009*\"get\" + 0.009*\"make\" + 0.008*\"don’t\" + 0.008*\"do\" + 0.008*\"covid\"'),\n",
       " (1,\n",
       "  '0.018*\"skype\" + 0.016*\"room\" + 0.012*\"updat\" + 0.010*\"dai\" + 0.010*\"histor\" + 0.009*\"new\" + 0.009*\"pineappl\" + 0.008*\"flower\" + 0.008*\"happi\" + 0.007*\"on\"'),\n",
       " (2,\n",
       "  '0.021*\"hostag\" + 0.019*\"video\" + 0.012*\"point\" + 0.012*\"it’s\" + 0.010*\"not\" + 0.010*\"on\" + 0.009*\"like\" + 0.009*\"book\" + 0.008*\"updat\" + 0.008*\"thing\"'),\n",
       " (3,\n",
       "  '0.048*\"art\" + 0.035*\"love\" + 0.027*\"light\" + 0.023*\"plant\" + 0.023*\"add\" + 0.023*\"good\" + 0.022*\"book\" + 0.020*\"nice\" + 0.020*\"well\" + 0.018*\"up\"'),\n",
       " (4,\n",
       "  '0.036*\"love\" + 0.021*\"art\" + 0.018*\"light\" + 0.015*\"plant\" + 0.012*\"colour\" + 0.010*\"depth\" + 0.010*\"game\" + 0.010*\"great\" + 0.010*\"lamp\" + 0.009*\"pillow\"')]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into LDA (5 TOPICS)\n",
    "\n",
    "# cleaned\n",
    "lda_5_cleaned = LdaModel(bag_of_words_cleaned, \n",
    "                    num_topics = 5,\n",
    "                    id2word = token_dict_cleaned,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "#print(lda_5_cleaned.show_topics())\n",
    "\n",
    "# stemmed\n",
    "lda_5_stemmed = LdaModel(bag_of_words_stemmed, \n",
    "                    num_topics = 5,\n",
    "                    id2word = token_dict_stemmed,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_5_stemmed.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-institute",
   "metadata": {},
   "source": [
    "### Fitting Cleaned + Stemmed data into LDA model with 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "heard-course",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.022*\"dai\" + 0.022*\"covid\" + 0.018*\"cabin\" + 0.016*\"pleas\" + 0.016*\"happi\" + 0.016*\"you’re\" + 0.014*\"canada\" + 0.012*\"can\" + 0.011*\"birthdai\" + 0.010*\"3\"'),\n",
       " (1,\n",
       "  '0.026*\"updat\" + 0.019*\"histor\" + 0.019*\"than\" + 0.018*\"pineappl\" + 0.015*\"flower\" + 0.012*\"get\" + 0.012*\"new\" + 0.012*\"thank\" + 0.011*\"live\" + 0.011*\"beam\"'),\n",
       " (2,\n",
       "  '0.031*\"too\" + 0.024*\"not\" + 0.020*\"like\" + 0.020*\"it’s\" + 0.016*\"help\" + 0.012*\"hostag\" + 0.011*\"dark\" + 0.011*\"if\" + 0.010*\"go\" + 0.009*\"ok\"'),\n",
       " (3,\n",
       "  '0.055*\"up\" + 0.054*\"book\" + 0.047*\"well\" + 0.035*\"nice\" + 0.034*\"good\" + 0.034*\"set\" + 0.023*\"lit\" + 0.016*\"point\" + 0.016*\"art\" + 0.013*\"shelf\"'),\n",
       " (4,\n",
       "  '0.026*\"violat\" + 0.024*\"cord\" + 0.016*\"solid\" + 0.015*\"expect\" + 0.013*\"vibe\" + 0.013*\"oh\" + 0.013*\"music\" + 0.010*\"not\" + 0.010*\"minor\" + 0.009*\"fan\"'),\n",
       " (5,\n",
       "  '0.039*\"hostag\" + 0.036*\"video\" + 0.019*\"deduct\" + 0.018*\"done\" + 0.016*\"best\" + 0.014*\"not\" + 0.014*\"try\" + 0.014*\"much\" + 0.012*\"big\" + 0.011*\"nail\"'),\n",
       " (6,\n",
       "  '0.071*\"skype\" + 0.064*\"room\" + 0.028*\"ad\" + 0.020*\"great\" + 0.016*\"librari\" + 0.015*\"close\" + 0.015*\"stage\" + 0.013*\"2\" + 0.012*\"ladder\" + 0.012*\"pretti\"'),\n",
       " (7,\n",
       "  '0.026*\"out\" + 0.016*\"like\" + 0.014*\"look\" + 0.012*\"point\" + 0.012*\"on\" + 0.011*\"crop\" + 0.011*\"nice\" + 0.010*\"plant\" + 0.010*\"that’s\" + 0.010*\"no\"'),\n",
       " (8,\n",
       "  '0.070*\"art\" + 0.061*\"love\" + 0.042*\"light\" + 0.031*\"plant\" + 0.029*\"add\" + 0.025*\"great\" + 0.020*\"camera\" + 0.020*\"lamp\" + 0.017*\"depth\" + 0.017*\"good\"'),\n",
       " (9,\n",
       "  '0.038*\"it’s\" + 0.016*\"know\" + 0.015*\"do\" + 0.014*\"not\" + 0.013*\"don’t\" + 0.013*\"see\" + 0.012*\"chang\" + 0.012*\"doesn’t\" + 0.011*\"let’s\" + 0.011*\"still\"')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into LDA (10 TOPICS)\n",
    "\n",
    "# cleaned\n",
    "lda_10_cleaned = LdaModel(bag_of_words_cleaned, \n",
    "                    num_topics = 10,\n",
    "                    id2word = token_dict_cleaned,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "#print(lda_5_cleaned.show_topics())\n",
    "\n",
    "# stemmed\n",
    "lda_10_stemmed = LdaModel(bag_of_words_stemmed, \n",
    "                    num_topics = 10,\n",
    "                    id2word = token_dict_stemmed,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_10_stemmed.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-familiar",
   "metadata": {},
   "source": [
    "### Fitting Cleaned + Stemmed data into LDA model with 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "portable-concentrate",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,\n",
       "  '0.101*\"love\" + 0.094*\"art\" + 0.076*\"great\" + 0.036*\"depth\" + 0.034*\"lighting\" + 0.021*\"plant\" + 0.021*\"flowers\" + 0.020*\"lamp\" + 0.017*\"chair\" + 0.015*\"blue\"'),\n",
       " (14,\n",
       "  '0.054*\"see\" + 0.031*\"so\" + 0.029*\"can\" + 0.028*\"space\" + 0.026*\"like\" + 0.023*\"more\" + 0.018*\"make\" + 0.018*\"would\" + 0.017*\"art\" + 0.017*\"fix\"'),\n",
       " (2,\n",
       "  '0.090*\"it’s\" + 0.038*\"always\" + 0.035*\"bad\" + 0.032*\"succulent\" + 0.026*\"way\" + 0.024*\"still\" + 0.021*\"top\" + 0.020*\"being\" + 0.016*\"dog\" + 0.015*\"position\"'),\n",
       " (16,\n",
       "  '0.080*\"out\" + 0.043*\"not\" + 0.029*\"ok\" + 0.029*\"crop\" + 0.026*\"he’s\" + 0.023*\"that’s\" + 0.022*\"ceiling\" + 0.018*\"doesn’t\" + 0.017*\"music\" + 0.017*\"sure\"'),\n",
       " (7,\n",
       "  '0.049*\"lovely\" + 0.030*\"than\" + 0.023*\"elegant\" + 0.021*\"cabinet\" + 0.019*\"open\" + 0.019*\"collection\" + 0.019*\"yellow\" + 0.019*\"nailed\" + 0.018*\"start\" + 0.017*\"update\"'),\n",
       " (3,\n",
       "  '0.025*\"there’s\" + 0.024*\"1\" + 0.021*\"we’ll\" + 0.020*\"we’ve\" + 0.017*\"@jheil\" + 0.016*\"seen\" + 0.015*\"yet\" + 0.015*\"someone\" + 0.014*\"help\" + 0.014*\"best\"'),\n",
       " (12,\n",
       "  '0.068*\"one\" + 0.057*\"just\" + 0.040*\"cool\" + 0.020*\"canada\" + 0.018*\"own\" + 0.018*\"book\" + 0.016*\"@brittlestar\" + 0.016*\"bigger\" + 0.015*\"almost\" + 0.015*\"keep\"'),\n",
       " (13,\n",
       "  '0.041*\"historic\" + 0.029*\"rooms\" + 0.026*\"got\" + 0.023*\"background\" + 0.023*\"photos\" + 0.022*\"lines\" + 0.022*\"change\" + 0.018*\"does\" + 0.015*\"these\" + 0.015*\"behind\"'),\n",
       " (9,\n",
       "  '0.066*\"books\" + 0.032*\"no\" + 0.027*\"could\" + 0.026*\"horizontal\" + 0.023*\"better\" + 0.022*\"real\" + 0.022*\"deduction\" + 0.021*\"even\" + 0.020*\"point\" + 0.017*\"oh\"'),\n",
       " (17,\n",
       "  '0.108*\"well\" + 0.094*\"up\" + 0.057*\"set\" + 0.047*\"nice\" + 0.046*\"good\" + 0.041*\"lit\" + 0.032*\"book\" + 0.024*\"books\" + 0.023*\"shelf\" + 0.016*\"shelves\"')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into LDA (20 TOPICS)\n",
    "lda_20 = LdaModel(bag_of_words_cleaned, \n",
    "                    num_topics = 20,\n",
    "                    id2word = token_dict,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_20.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "palestinian-channel",
   "metadata": {},
   "source": [
    "### For each cleaned + stemmed data, apply both 5 and 10 topic models to each tweet, output 5 topic and 10 topic probabilities for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "interior-thing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010069694,\n",
       " 0.66924024,\n",
       " 0.010069694,\n",
       " 0.25019646,\n",
       " 0.010071237,\n",
       " 0.010070478,\n",
       " 0.010069693,\n",
       " 0.010069954,\n",
       " 0.010070965,\n",
       " 0.010071561]"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_topic_probabilities_cleaned(tokens, num_topics):\n",
    "    bow = token_dict_cleaned.doc2bow(tokens)\n",
    "    if num_topics == 10: \n",
    "        topic_probs = lda_10_cleaned.get_document_topics(bow)\n",
    "    if num_topics == 5:\n",
    "        topic_probs = lda_5_cleaned.get_document_topics(bow)\n",
    "        \n",
    "    probs = []\n",
    "        \n",
    "    probs_dict = dict(topic_probs)\n",
    "        \n",
    "    for i in range (0, num_topics):\n",
    "        if i not in probs_dict: \n",
    "            probs.append(0)\n",
    "        else: \n",
    "            probs.append(probs_dict[i])\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def extract_topic_probabilities_stemmed(tokens, num_topics):\n",
    "    bow = token_dict_stemmed.doc2bow(tokens)\n",
    "    if num_topics == 10: \n",
    "        topic_probs = lda_10_stemmed.get_document_topics(bow)\n",
    "    if num_topics == 5:\n",
    "        topic_probs = lda_5_stemmed.get_document_topics(bow)\n",
    "        \n",
    "    probs = []\n",
    "        \n",
    "    probs_dict = dict(topic_probs)\n",
    "        \n",
    "    for i in range (0, num_topics):\n",
    "        if i not in probs_dict: \n",
    "            probs.append(0)\n",
    "        else: \n",
    "            probs.append(probs_dict[i])\n",
    "    \n",
    "    return probs\n",
    "   \n",
    "extract_topic_probabilities_cleaned(processed, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "verbal-sheffield",
   "metadata": {},
   "source": [
    "### Adding topic probabilities as columns to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "macro-truth",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "preprocessed['lda_10_probs_cleaned'] = preprocessed['cleaned'].apply(lambda x : extract_topic_probabilities_cleaned(x, 10))\n",
    "preprocessed['lda_5_probs_cleaned'] = preprocessed['cleaned'].apply(lambda x : extract_topic_probabilities_cleaned(x, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "swedish-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['lda_10_probs_stemmed'] = preprocessed['stemmed'].apply(lambda x : extract_topic_probabilities_stemmed(x, 10))\n",
    "preprocessed['lda_5_probs_stemmed'] = preprocessed['stemmed'].apply(lambda x : extract_topic_probabilities_stemmed(x, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "pressed-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tags_removed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>lda_10_probs</th>\n",
       "      <th>lda_5_probs</th>\n",
       "      <th>lda_10_probs_cleaned</th>\n",
       "      <th>lda_5_probs_cleaned</th>\n",
       "      <th>lda_10_probs_stemmed</th>\n",
       "      <th>lda_5_probs_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>[new, view, love, art, present, perspect, ligh...</td>\n",
       "      <td>[0, 0.6853188, 0, 0.24769051, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.11110465, 0.8379315, 0.016782343, 0.0173785...</td>\n",
       "      <td>[0, 0.68527937, 0, 0.24772996, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.11110278, 0.8379478, 0.016782334, 0.0173642...</td>\n",
       "      <td>[0, 0, 0, 0.14467387, 0, 0, 0, 0, 0.7753759, 0]</td>\n",
       "      <td>[0.018320857, 0.018461967, 0.018315086, 0.9264...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>[st, paddy’s, dai, edit, great, wall, color, m...</td>\n",
       "      <td>[0, 0.11529674, 0, 0.57690215, 0, 0, 0, 0.2611...</td>\n",
       "      <td>[0.013348684, 0.31573728, 0.0128906, 0.5755992...</td>\n",
       "      <td>[0, 0.11548184, 0, 0.5767406, 0, 0, 0, 0.26113...</td>\n",
       "      <td>[0.013347856, 0.31579176, 0.012890519, 0.57554...</td>\n",
       "      <td>[0, 0.31566647, 0, 0, 0, 0, 0, 0, 0.6309494, 0]</td>\n",
       "      <td>[0.013394717, 0.3650373, 0.013496481, 0.594382...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>[door, dublin, dingus-the, stuff, dream, made,...</td>\n",
       "      <td>[0.19799763, 0.2285615, 0, 0, 0, 0, 0.12228879...</td>\n",
       "      <td>[0.7600076, 0.1963269, 0.014439243, 0.01468350...</td>\n",
       "      <td>[0.19799131, 0.22857866, 0, 0, 0, 0, 0.1222571...</td>\n",
       "      <td>[0.75999737, 0.19633839, 0.014439249, 0.014682...</td>\n",
       "      <td>[0, 0.057735033, 0, 0.54084694, 0, 0.1300493, ...</td>\n",
       "      <td>[0.10219681, 0.19941911, 0.014534805, 0.584886...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[0, 0, 0, 0.60546404, 0.33275887, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.015479421, 0.01595111, 0.015447192, 0.93746...</td>\n",
       "      <td>[0, 0, 0, 0.6054711, 0.3327518, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.015479418, 0.015946027, 0.015447189, 0.9374...</td>\n",
       "      <td>[0, 0, 0, 0.4312952, 0, 0, 0, 0, 0.50202626, 0]</td>\n",
       "      <td>[0.016847817, 0.01667914, 0.016725928, 0.93293...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>[dark, never, escap, stank, wrong, about, ever...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0.4000559, 0.38573247, 0.15...</td>\n",
       "      <td>[0.018459236, 0.018510584, 0.77921337, 0.01831...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0.39992207, 0.38575116, 0.1...</td>\n",
       "      <td>[0.018459152, 0.018509388, 0.7792248, 0.018318...</td>\n",
       "      <td>[0.01018724, 0.010193234, 0.2553968, 0.3359198...</td>\n",
       "      <td>[0.38353115, 0.5552803, 0.020458056, 0.0202665...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "0       9  [new, view, love, art, presentation, perspecti...   \n",
       "1       9  [st, paddy’s, day, edition, great, wall, color...   \n",
       "2      10  [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3       9  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4       0  [dark, never, escape, stank, wrong, about, eve...   \n",
       "\n",
       "                                        tags_removed  \\\n",
       "0  [new, view, love, art, presentation, perspecti...   \n",
       "1  [st, paddy’s, day, edition, great, wall, color...   \n",
       "2  [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4  [dark, never, escape, stank, wrong, about, eve...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [new, view, love, art, present, perspect, ligh...   \n",
       "1  [st, paddy’s, dai, edit, great, wall, color, m...   \n",
       "2  [door, dublin, dingus-the, stuff, dream, made,...   \n",
       "3  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4  [dark, never, escap, stank, wrong, about, ever...   \n",
       "\n",
       "                                        lda_10_probs  \\\n",
       "0    [0, 0.6853188, 0, 0.24769051, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0.11529674, 0, 0.57690215, 0, 0, 0, 0.2611...   \n",
       "2  [0.19799763, 0.2285615, 0, 0, 0, 0, 0.12228879...   \n",
       "3   [0, 0, 0, 0.60546404, 0.33275887, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0.4000559, 0.38573247, 0.15...   \n",
       "\n",
       "                                         lda_5_probs  \\\n",
       "0  [0.11110465, 0.8379315, 0.016782343, 0.0173785...   \n",
       "1  [0.013348684, 0.31573728, 0.0128906, 0.5755992...   \n",
       "2  [0.7600076, 0.1963269, 0.014439243, 0.01468350...   \n",
       "3  [0.015479421, 0.01595111, 0.015447192, 0.93746...   \n",
       "4  [0.018459236, 0.018510584, 0.77921337, 0.01831...   \n",
       "\n",
       "                                lda_10_probs_cleaned  \\\n",
       "0   [0, 0.68527937, 0, 0.24772996, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0.11548184, 0, 0.5767406, 0, 0, 0, 0.26113...   \n",
       "2  [0.19799131, 0.22857866, 0, 0, 0, 0, 0.1222571...   \n",
       "3     [0, 0, 0, 0.6054711, 0.3327518, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0.39992207, 0.38575116, 0.1...   \n",
       "\n",
       "                                 lda_5_probs_cleaned  \\\n",
       "0  [0.11110278, 0.8379478, 0.016782334, 0.0173642...   \n",
       "1  [0.013347856, 0.31579176, 0.012890519, 0.57554...   \n",
       "2  [0.75999737, 0.19633839, 0.014439249, 0.014682...   \n",
       "3  [0.015479418, 0.015946027, 0.015447189, 0.9374...   \n",
       "4  [0.018459152, 0.018509388, 0.7792248, 0.018318...   \n",
       "\n",
       "                                lda_10_probs_stemmed  \\\n",
       "0    [0, 0, 0, 0.14467387, 0, 0, 0, 0, 0.7753759, 0]   \n",
       "1    [0, 0.31566647, 0, 0, 0, 0, 0, 0, 0.6309494, 0]   \n",
       "2  [0, 0.057735033, 0, 0.54084694, 0, 0.1300493, ...   \n",
       "3    [0, 0, 0, 0.4312952, 0, 0, 0, 0, 0.50202626, 0]   \n",
       "4  [0.01018724, 0.010193234, 0.2553968, 0.3359198...   \n",
       "\n",
       "                                 lda_5_probs_stemmed  \n",
       "0  [0.018320857, 0.018461967, 0.018315086, 0.9264...  \n",
       "1  [0.013394717, 0.3650373, 0.013496481, 0.594382...  \n",
       "2  [0.10219681, 0.19941911, 0.014534805, 0.584886...  \n",
       "3  [0.016847817, 0.01667914, 0.016725928, 0.93293...  \n",
       "4  [0.38353115, 0.5552803, 0.020458056, 0.0202665...  "
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-miller",
   "metadata": {},
   "source": [
    "### Creating dataframe for each number of topics and preprocessing pair, each dataframe has rating (y) and columns for each topic probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "caring-auction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/pandas/core/frame.py:3188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.018321</td>\n",
       "      <td>0.018462</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.926455</td>\n",
       "      <td>0.018447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.013395</td>\n",
       "      <td>0.365037</td>\n",
       "      <td>0.013496</td>\n",
       "      <td>0.594382</td>\n",
       "      <td>0.013690</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.102197</td>\n",
       "      <td>0.199419</td>\n",
       "      <td>0.014535</td>\n",
       "      <td>0.584886</td>\n",
       "      <td>0.098963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.016679</td>\n",
       "      <td>0.016726</td>\n",
       "      <td>0.932938</td>\n",
       "      <td>0.016809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.383531</td>\n",
       "      <td>0.555280</td>\n",
       "      <td>0.020458</td>\n",
       "      <td>0.020267</td>\n",
       "      <td>0.020464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating         0         1         2         3         4\n",
       "0       9  0.018321  0.018462  0.018315  0.926455  0.018447\n",
       "1       9  0.013395  0.365037  0.013496  0.594382  0.013690\n",
       "2      10  0.102197  0.199419  0.014535  0.584886  0.098963\n",
       "3       9  0.016848  0.016679  0.016726  0.932938  0.016809\n",
       "4       0  0.383531  0.555280  0.020458  0.020267  0.020464"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 TOPICS: CLEANED\n",
    "lda_features_df_10_cleaned = preprocessed[['rating', 'lda_10_probs_cleaned']]\n",
    "lda_features_df_10_cleaned[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']] = pd.DataFrame(lda_features_df_10_cleaned.lda_10_probs_cleaned.tolist(), index=lda_features_df_10_cleaned.index)\n",
    "lda_features_df_10_cleaned = lda_features_df_10_cleaned[['rating', '0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "lda_features_df_10_cleaned\n",
    "\n",
    "# 10 TOPICS: STEMMED\n",
    "lda_features_df_10_stemmed = preprocessed[['rating', 'lda_10_probs_stemmed']]\n",
    "lda_features_df_10_stemmed[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']] = pd.DataFrame(lda_features_df_10_stemmed.lda_10_probs_stemmed.tolist(), index=lda_features_df_10_stemmed.index)\n",
    "lda_features_df_10_stemmed = lda_features_df_10_stemmed[['rating', '0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "lda_features_df_10_stemmed\n",
    "\n",
    "# 5 TOPICS: CLEANED\n",
    "lda_features_df_5_cleaned = preprocessed[['rating', 'lda_5_probs_cleaned']]\n",
    "lda_features_df_5_cleaned[['0','1', '2', '3', '4']] = pd.DataFrame(lda_features_df_5_cleaned.lda_5_probs_cleaned.tolist(), index=lda_features_df_5_cleaned.index)\n",
    "lda_features_df_5_cleaned = lda_features_df_5_cleaned[['rating', '0','1', '2', '3', '4']]\n",
    "lda_features_df_5_cleaned\n",
    "\n",
    "# 5 TOPICS: STEMMED\n",
    "lda_features_df_5_stemmed = preprocessed[['rating', 'lda_5_probs_stemmed']]\n",
    "lda_features_df_5_stemmed[['0','1', '2', '3', '4']] = pd.DataFrame(lda_features_df_5_stemmed.lda_5_probs_stemmed.tolist(), index=lda_features_df_5_stemmed.index)\n",
    "lda_features_df_5_stemmed = lda_features_df_5_stemmed[['rating', '0','1', '2', '3', '4']]\n",
    "lda_features_df_5_stemmed.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cellular-evans",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/pandas/core/frame.py:3188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.111105</td>\n",
       "      <td>0.837932</td>\n",
       "      <td>0.016782</td>\n",
       "      <td>0.017379</td>\n",
       "      <td>0.016803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.013349</td>\n",
       "      <td>0.315737</td>\n",
       "      <td>0.012891</td>\n",
       "      <td>0.575599</td>\n",
       "      <td>0.082424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.760008</td>\n",
       "      <td>0.196327</td>\n",
       "      <td>0.014439</td>\n",
       "      <td>0.014684</td>\n",
       "      <td>0.014543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.015479</td>\n",
       "      <td>0.015951</td>\n",
       "      <td>0.015447</td>\n",
       "      <td>0.937465</td>\n",
       "      <td>0.015657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.018459</td>\n",
       "      <td>0.018511</td>\n",
       "      <td>0.779213</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.165498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating         0         1         2         3         4\n",
       "0       9  0.111105  0.837932  0.016782  0.017379  0.016803\n",
       "1       9  0.013349  0.315737  0.012891  0.575599  0.082424\n",
       "2      10  0.760008  0.196327  0.014439  0.014684  0.014543\n",
       "3       9  0.015479  0.015951  0.015447  0.937465  0.015657\n",
       "4       0  0.018459  0.018511  0.779213  0.018319  0.165498"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_features_df_5 = preprocessed[['rating', 'lda_5_probs']]\n",
    "lda_features_df_5[['0','1', '2', '3', '4']] = pd.DataFrame(lda_features_df_5.lda_5_probs.tolist(), index=lda_features_df_5.index)\n",
    "lda_features_df_5 = lda_features_df_5[['rating', '0','1', '2', '3', '4']]\n",
    "lda_features_df_5.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-battery",
   "metadata": {},
   "source": [
    "### Fitting Logistic Regression Model on above dataframes, features (X) = topic probabilities, output (y) = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "falling-perth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED SCORES\n",
      "10 cleaned training acc:  0.44329896907216493\n",
      "10 cleaned testing acc:  0.2841169219933087\n",
      "10 cleaned testing F1 score:  0.029497076962185495\n",
      "10 cleaned testing roc score:  0.5055781987019304\n",
      "STEMMED SCORES\n",
      "10 stemmed training acc:  0.44885011895321175\n",
      "10 stemmed testing acc:  0.29961260785349536\n",
      "10 stemmed testing F1 score:  0.03895414416761144\n",
      "10 stemmed testing roc score:  0.5124778559464753\n"
     ]
    }
   ],
   "source": [
    "# (10 TOPICS)\n",
    "\n",
    "# CLEANED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_10_cleaned))\n",
    "train = lda_features_df_10_cleaned[:train_size]\n",
    "test = lda_features_df_10_cleaned[train_size:]\n",
    "\n",
    "X_all = lda_features_df_10_cleaned[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_all = lda_features_df_10_cleaned['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_test = test['rating']\n",
    "\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "logm_10_cleaned = LogisticRegression()\n",
    "logm_10_cleaned.fit(X_train, y_train)\n",
    "\n",
    "print(\"CLEANED SCORES\")\n",
    "\n",
    "print(\"10 cleaned training acc: \", logm_10_cleaned.score(X_train, y_train))\n",
    "print(\"10 cleaned testing acc: \", logm_10_cleaned.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_10_cleaned.predict(X_test)\n",
    "print(\"10 cleaned testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"10 cleaned testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n",
    "# STEMMED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_10_stemmed))\n",
    "train = lda_features_df_10_stemmed[:train_size]\n",
    "test = lda_features_df_10_stemmed[train_size:]\n",
    "\n",
    "X_all = lda_features_df_10_stemmed[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_all = lda_features_df_10_stemmed['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_10_stemmed = LogisticRegression()\n",
    "logm_10_stemmed.fit(X_train, y_train)\n",
    "\n",
    "print(\"STEMMED SCORES\")\n",
    "\n",
    "print(\"10 stemmed training acc: \", logm_10_stemmed.score(X_train, y_train))\n",
    "print(\"10 stemmed testing acc: \", logm_10_stemmed.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_10_stemmed.predict(X_test)\n",
    "print(\"10 stemmed testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"10 stemmed testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "stainless-framing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED SCORES\n",
      "5 cleaned training score:  0.4274385408406027\n",
      "5 cleaned testing score:  0.2900158478605388\n",
      "5 cleaned testing F1 score:  0.030602484476379446\n",
      "5 cleaned testing roc score:  0.5072406203665009\n",
      "STEMMED SCORES\n",
      "5 stemmed training score:  0.43536875495638383\n",
      "5 stemmed testing score:  0.30198978693431944\n",
      "5 stemmed testing F1 score:  0.033609750498231734\n",
      "5 stemmed testing roc score:  0.5108904819148777\n"
     ]
    }
   ],
   "source": [
    "# fitting logistic regression model on training data (5 TOPICS)\n",
    "\n",
    "# CLEANED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_5_cleaned))\n",
    "train = lda_features_df_5_cleaned[:train_size]\n",
    "test = lda_features_df_5_cleaned[train_size:]\n",
    "\n",
    "X_all = lda_features_df_5_cleaned[['0','1', '2', '3', '4']]\n",
    "y_all = lda_features_df_5_cleaned['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_5_cleaned = LogisticRegression()\n",
    "logm_5_cleaned.fit(X_train, y_train)\n",
    "\n",
    "print(\"CLEANED SCORES\")\n",
    "\n",
    "print(\"5 cleaned training score: \", logm_5_cleaned.score(X_train, y_train))\n",
    "print(\"5 cleaned testing score: \", logm_5_cleaned.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_5_cleaned.predict(X_test)\n",
    "print(\"5 cleaned testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"5 cleaned testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n",
    "# STEMMED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_5_stemmed))\n",
    "train = lda_features_df_5_stemmed[:train_size]\n",
    "test = lda_features_df_5_stemmed[train_size:]\n",
    "\n",
    "X_all = lda_features_df_5_stemmed[['0','1', '2', '3', '4']]\n",
    "y_all = lda_features_df_5_stemmed['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_5_stemmed = LogisticRegression()\n",
    "logm_5_stemmed.fit(X_train, y_train)\n",
    "\n",
    "print(\"STEMMED SCORES\")\n",
    "\n",
    "print(\"5 stemmed training score: \", logm_5_stemmed.score(X_train, y_train))\n",
    "print(\"5 stemmed testing score: \", logm_5_stemmed.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_5_stemmed.predict(X_test)\n",
    "print(\"5 stemmed testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"5 stemmed testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-athletics",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
