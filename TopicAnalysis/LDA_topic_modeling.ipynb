{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "stretch-exposure",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import string\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "import gensim\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "latin-cleaner",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = pd.read_csv(\"../data/cleaned.csv\")\n",
    "images = pd.read_csv(\"../data/images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "curious-villa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>bigram</th>\n",
       "      <th>tweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>new view love art presentation perspective lig...</td>\n",
       "      <td>newview viewlove loveart artpresentation prese...</td>\n",
       "      <td>New view. Love the art. Presentation. Perspect...</td>\n",
       "      <td>2021-03-17 16:24:35 EDT</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>62</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtT4E7UUAAgt5w.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>st paddy’s day edition great wall color maps a...</td>\n",
       "      <td>stpaddy paddy’ ’s sday dayedition editiongreat...</td>\n",
       "      <td>Room Rater St Paddy’s Day Edition. Great wall ...</td>\n",
       "      <td>2021-03-17 15:25:51 EDT</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>57</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtGbzTUYAAxDiu.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>doors dublin dingus-the stuff dreams made nice...</td>\n",
       "      <td>doorsdublin dublindingus-the dingus-thestuff s...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "      <td>2021-03-17 15:19:53 EDT</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>good tight set up love blue art flag widen sho...</td>\n",
       "      <td>goodtight tightset setup uplove loveblue bluea...</td>\n",
       "      <td>Good tight set up. Love the blue. Art. Flag. W...</td>\n",
       "      <td>2021-03-17 13:46:47 EDT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>https://pbs.twimg.com/media/EwsvwlOU8AQ5lOY.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>dark never escape stank wrong about everything...</td>\n",
       "      <td>darknever neverescape escapestank stankwrong w...</td>\n",
       "      <td>Dark. Will never escape the stank. Wrong about...</td>\n",
       "      <td>2021-03-17 13:11:17 EDT</td>\n",
       "      <td>35</td>\n",
       "      <td>38</td>\n",
       "      <td>846</td>\n",
       "      <td>https://pbs.twimg.com/media/Ewsnoq5U8AMxxAg.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "0       9  new view love art presentation perspective lig...   \n",
       "1       9  st paddy’s day edition great wall color maps a...   \n",
       "2      10  doors dublin dingus-the stuff dreams made nice...   \n",
       "3       9  good tight set up love blue art flag widen sho...   \n",
       "4       0  dark never escape stank wrong about everything...   \n",
       "\n",
       "                                              bigram  \\\n",
       "0  newview viewlove loveart artpresentation prese...   \n",
       "1  stpaddy paddy’ ’s sday dayedition editiongreat...   \n",
       "2  doorsdublin dublindingus-the dingus-thestuff s...   \n",
       "3  goodtight tightset setup uplove loveblue bluea...   \n",
       "4  darknever neverescape escapestank stankwrong w...   \n",
       "\n",
       "                                               tweet               created_at  \\\n",
       "0  New view. Love the art. Presentation. Perspect...  2021-03-17 16:24:35 EDT   \n",
       "1  Room Rater St Paddy’s Day Edition. Great wall ...  2021-03-17 15:25:51 EDT   \n",
       "2  Doors of Dublin. The Dingus-the stuff that dre...  2021-03-17 15:19:53 EDT   \n",
       "3  Good tight set up. Love the blue. Art. Flag. W...  2021-03-17 13:46:47 EDT   \n",
       "4  Dark. Will never escape the stank. Wrong about...  2021-03-17 13:11:17 EDT   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  \\\n",
       "0              3               2           62   \n",
       "1              2               2           57   \n",
       "2              7               4          123   \n",
       "3              1               1           60   \n",
       "4             35              38          846   \n",
       "\n",
       "                                           img_url  \n",
       "0  https://pbs.twimg.com/media/EwtT4E7UUAAgt5w.jpg  \n",
       "1  https://pbs.twimg.com/media/EwtGbzTUYAAxDiu.jpg  \n",
       "2  https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg  \n",
       "3  https://pbs.twimg.com/media/EwsvwlOU8AQ5lOY.jpg  \n",
       "4  https://pbs.twimg.com/media/Ewsnoq5U8AMxxAg.jpg  "
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "fatal-pavilion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'New view. Love the art. Presentation. Perspective. Light/lighting. Widen shot slightly. 9/10 @DavidJollyFL  https://t.co/oZJcRkTSJd'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.iloc[0][\"tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "deluxe-works",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book club love art lighting flowers arkansas traveler @iamsophianelson'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned.iloc[7][\"cleaned\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "sharp-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>bigram</th>\n",
       "      <th>tweet</th>\n",
       "      <th>created_at</th>\n",
       "      <th>replies_count</th>\n",
       "      <th>retweets_count</th>\n",
       "      <th>likes_count</th>\n",
       "      <th>img_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>doors dublin dingus-the stuff dreams made nice...</td>\n",
       "      <td>doorsdublin dublindingus-the dingus-thestuff s...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "      <td>2021-03-17 15:19:53 EDT</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>123</td>\n",
       "      <td>https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>st paddy’s day update @philiprucker green tie</td>\n",
       "      <td>stpaddy paddy’ ’s sday dayupdate update@ @phil...</td>\n",
       "      <td>Room Rater St Paddy’s Day Update. @PhilipRucke...</td>\n",
       "      <td>2021-03-17 12:34:31 EDT</td>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>620</td>\n",
       "      <td>https://pbs.twimg.com/media/EwsfN-yVoAAcVk8.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>book club love art lighting flowers arkansas t...</td>\n",
       "      <td>bookclub clublove loveart artlighting lighting...</td>\n",
       "      <td>Room Rater Book Club. Love the art. Lighting. ...</td>\n",
       "      <td>2021-03-17 11:49:52 EDT</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>87</td>\n",
       "      <td>https://pbs.twimg.com/media/EwsVABRVgAEJGvE.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "2      10  doors dublin dingus-the stuff dreams made nice...   \n",
       "5      10      st paddy’s day update @philiprucker green tie   \n",
       "7      10  book club love art lighting flowers arkansas t...   \n",
       "\n",
       "                                              bigram  \\\n",
       "2  doorsdublin dublindingus-the dingus-thestuff s...   \n",
       "5  stpaddy paddy’ ’s sday dayupdate update@ @phil...   \n",
       "7  bookclub clublove loveart artlighting lighting...   \n",
       "\n",
       "                                               tweet               created_at  \\\n",
       "2  Doors of Dublin. The Dingus-the stuff that dre...  2021-03-17 15:19:53 EDT   \n",
       "5  Room Rater St Paddy’s Day Update. @PhilipRucke...  2021-03-17 12:34:31 EDT   \n",
       "7  Room Rater Book Club. Love the art. Lighting. ...  2021-03-17 11:49:52 EDT   \n",
       "\n",
       "   replies_count  retweets_count  likes_count  \\\n",
       "2              7               4          123   \n",
       "5             10              12          620   \n",
       "7              1               4           87   \n",
       "\n",
       "                                           img_url  \n",
       "2  https://pbs.twimg.com/media/EwtFESHVEAEYlzv.jpg  \n",
       "5  https://pbs.twimg.com/media/EwsfN-yVoAAcVk8.jpg  \n",
       "7  https://pbs.twimg.com/media/EwsVABRVgAEJGvE.jpg  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lowest rating\n",
    "low_rating = cleaned[cleaned['rating'] == 0]\n",
    "low_rating.head(3) # yields row 4, 20, 28\n",
    "\n",
    "# semi low rating\n",
    "semi_low_rating = cleaned[cleaned['rating'] == 3]\n",
    "semi_low_rating.head(3) # yields row 75, 135, 153\n",
    "\n",
    "# middle rating\n",
    "middle_rating = cleaned[cleaned['rating'] == 5]\n",
    "middle_rating.head(3) # yields rows 169, 240, 245\n",
    "\n",
    "#semi middle rating\n",
    "semi_middle_rating = cleaned[cleaned['rating'] == 7]\n",
    "semi_middle_rating.head(3) # yields rows 37, 45, 69\n",
    "\n",
    "# semi high rating\n",
    "semi_high_rating = cleaned[cleaned['rating'] == 8]\n",
    "semi_high_rating.head(3) # yields rows 8, 9, 17\n",
    "\n",
    "# highest rating\n",
    "highest_rating = cleaned[cleaned['rating'] == 10]\n",
    "highest_rating.head(3) # yields rows 2, 5, 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "korean-nursing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12619"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "declared-portal",
   "metadata": {},
   "source": [
    "## **ADDITIONAL PREPROCESSING:**\n",
    "    * Removing user tags (starting with @)\n",
    "    * Stemming each cleaned word in tweet\n",
    "    * Add column for default tokens (without removing stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "dying-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed = cleaned[['rating','cleaned', 'tweet']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "centered-speed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>new view love art presentation perspective lig...</td>\n",
       "      <td>New view. Love the art. Presentation. Perspect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>st paddy’s day edition great wall color maps a...</td>\n",
       "      <td>Room Rater St Paddy’s Day Edition. Great wall ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>doors dublin dingus-the stuff dreams made nice...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>good tight set up love blue art flag widen sho...</td>\n",
       "      <td>Good tight set up. Love the blue. Art. Flag. W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>dark never escape stank wrong about everything...</td>\n",
       "      <td>Dark. Will never escape the stank. Wrong about...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>st paddy’s day update @philiprucker green tie</td>\n",
       "      <td>Room Rater St Paddy’s Day Update. @PhilipRucke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>sun set ups tough raise camera late christmas ...</td>\n",
       "      <td>Sun room set ups are tough. Raise camera. Late...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>book club love art lighting flowers arkansas t...</td>\n",
       "      <td>Room Rater Book Club. Love the art. Lighting. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>ducks packers big plant well composed sports s...</td>\n",
       "      <td>Ducks. Packers. Big plant. Well composed sport...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>well composed set up good spacing lower camera...</td>\n",
       "      <td>Well composed set up. Good spacing. Lower came...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "0       9  new view love art presentation perspective lig...   \n",
       "1       9  st paddy’s day edition great wall color maps a...   \n",
       "2      10  doors dublin dingus-the stuff dreams made nice...   \n",
       "3       9  good tight set up love blue art flag widen sho...   \n",
       "4       0  dark never escape stank wrong about everything...   \n",
       "5      10      st paddy’s day update @philiprucker green tie   \n",
       "6       6  sun set ups tough raise camera late christmas ...   \n",
       "7      10  book club love art lighting flowers arkansas t...   \n",
       "8       8  ducks packers big plant well composed sports s...   \n",
       "9       8  well composed set up good spacing lower camera...   \n",
       "\n",
       "                                               tweet  \n",
       "0  New view. Love the art. Presentation. Perspect...  \n",
       "1  Room Rater St Paddy’s Day Edition. Great wall ...  \n",
       "2  Doors of Dublin. The Dingus-the stuff that dre...  \n",
       "3  Good tight set up. Love the blue. Art. Flag. W...  \n",
       "4  Dark. Will never escape the stank. Wrong about...  \n",
       "5  Room Rater St Paddy’s Day Update. @PhilipRucke...  \n",
       "6  Sun room set ups are tough. Raise camera. Late...  \n",
       "7  Room Rater Book Club. Love the art. Lighting. ...  \n",
       "8  Ducks. Packers. Big plant. Well composed sport...  \n",
       "9  Well composed set up. Good spacing. Lower came...  "
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "romance-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(tweet):\n",
    "    if pd.isnull(tweet):\n",
    "        return []\n",
    "    #print(tweet)\n",
    "    tokens = tweet.split(' ')\n",
    "    tokens = list(filter(lambda x: '@' not in x, tokens))\n",
    "    return tokens\n",
    "\n",
    "def stemWords(tokens):\n",
    "    result = []\n",
    "    if tokens is None:\n",
    "        return tokens\n",
    "    for token in tokens:\n",
    "        if token.isalpha():\n",
    "            stem = PorterStemmer()\n",
    "            stemmed_word = stem.stem(token, 0, len(token) - 1)\n",
    "            result.append(stemmed_word)\n",
    "        else: \n",
    "            result.append(token)\n",
    "\n",
    "    return result\n",
    "\n",
    "def split(tweet):\n",
    "    if pd.isnull(tweet):\n",
    "        return []\n",
    "    tokens = tweet.split(' ')\n",
    "    return tokens\n",
    "\n",
    "def strip_ratings(tweet):\n",
    "    tweet = re.sub(rating_pattern, \"\", tweet)\n",
    "    tweet = re.sub(\"[Mm]inus\", \"\", tweet)\n",
    "    return tweet\n",
    "\n",
    "def strip_links(tweet):\n",
    "    tweet = re.sub(\"https?://t.co/\\w+\", \"\", tweet)\n",
    "    return tweet\n",
    "\n",
    "def strip_end_punctuation_and_lower(tweet):\n",
    "    tweet_list = tweet.split()\n",
    "    return ' '.join([i.lower().rstrip(string.punctuation) for i in tweet_list])\n",
    "\n",
    "def default_preprocess(tweet):\n",
    "    tweet = strip_ratings(tweet)\n",
    "    tweet = strip_links(tweet)\n",
    "    tweet = strip_end_punctuation_and_lower(tweet) \n",
    "    tokens = tweet.split(\" \")\n",
    "    return tokens\n",
    "\n",
    "rating_pattern = re.compile(r\"((- ?)?\\d+)/10\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "meaning-mount",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "preprocessed['tags_removed'] = preprocessed['cleaned'].apply(lambda x : remove_tags(x))\n",
    "preprocessed['stemmed'] = preprocessed['tags_removed'].apply(lambda x : stemWords(x))\n",
    "preprocessed['cleaned'] = preprocessed['cleaned'].apply(lambda x : split(x))\n",
    "preprocessed['default'] = preprocessed['tweet'].apply(lambda x : default_preprocess(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "planned-jackson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tags_removed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>New view. Love the art. Presentation. Perspect...</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>[new, view, love, art, present, perspect, ligh...</td>\n",
       "      <td>[new, view, love, the, art, presentation, pers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>Room Rater St Paddy’s Day Edition. Great wall ...</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>[st, paddy’s, dai, edit, great, wall, color, m...</td>\n",
       "      <td>[room, rater, st, paddy’s, day, edition, great...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>[door, dublin, dingus-the, stuff, dream, made,...</td>\n",
       "      <td>[doors, of, dublin, the, dingus-the, stuff, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>Good tight set up. Love the blue. Art. Flag. W...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, the, blue, art, f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>Dark. Will never escape the stank. Wrong about...</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>[dark, never, escap, stank, wrong, about, ever...</td>\n",
       "      <td>[dark, will, never, escape, the, stank, wrong,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12614</th>\n",
       "      <td>8</td>\n",
       "      <td>[blue, works, not, just, books, plus, good, us...</td>\n",
       "      <td>The blue works. Not just books a plus. Good us...</td>\n",
       "      <td>[blue, works, not, just, books, plus, good, us...</td>\n",
       "      <td>[blue, work, not, just, book, plu, good, us, s...</td>\n",
       "      <td>[the, blue, works, not, just, books, a, plus, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12615</th>\n",
       "      <td>2</td>\n",
       "      <td>[“i’ll, just, put, sweatpants”, skype, rooms]</td>\n",
       "      <td>This is the “I’ll just put on sweatpants” of S...</td>\n",
       "      <td>[“i’ll, just, put, sweatpants”, skype, rooms]</td>\n",
       "      <td>[“i’ll, just, put, sweatpants”, skype, room]</td>\n",
       "      <td>[this, is, the, “i’ll, just, put, on, sweatpan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12616</th>\n",
       "      <td>3</td>\n",
       "      <td>[books, too, dark, little, way, personal, style]</td>\n",
       "      <td>All books. Too dark. Little in way of personal...</td>\n",
       "      <td>[books, too, dark, little, way, personal, style]</td>\n",
       "      <td>[book, too, dark, littl, wai, person, style]</td>\n",
       "      <td>[all, books, too, dark, little, in, way, of, p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12617</th>\n",
       "      <td>4</td>\n",
       "      <td>[books, always, must, little, too, obvious, su...</td>\n",
       "      <td>Books always a must but a little too obvious f...</td>\n",
       "      <td>[books, always, must, little, too, obvious, su...</td>\n",
       "      <td>[book, alwai, must, littl, too, obviou, such, ...</td>\n",
       "      <td>[books, always, a, must, but, a, little, too, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12618</th>\n",
       "      <td>2</td>\n",
       "      <td>[she’s, not, even, trying]</td>\n",
       "      <td>She’s not even trying. 2/10.  https://t.co/MrA...</td>\n",
       "      <td>[she’s, not, even, trying]</td>\n",
       "      <td>[she’s, not, even, try]</td>\n",
       "      <td>[she’s, not, even, trying, ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12619 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rating                                            cleaned  \\\n",
       "0           9  [new, view, love, art, presentation, perspecti...   \n",
       "1           9  [st, paddy’s, day, edition, great, wall, color...   \n",
       "2          10  [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3           9  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4           0  [dark, never, escape, stank, wrong, about, eve...   \n",
       "...       ...                                                ...   \n",
       "12614       8  [blue, works, not, just, books, plus, good, us...   \n",
       "12615       2      [“i’ll, just, put, sweatpants”, skype, rooms]   \n",
       "12616       3   [books, too, dark, little, way, personal, style]   \n",
       "12617       4  [books, always, must, little, too, obvious, su...   \n",
       "12618       2                         [she’s, not, even, trying]   \n",
       "\n",
       "                                                   tweet  \\\n",
       "0      New view. Love the art. Presentation. Perspect...   \n",
       "1      Room Rater St Paddy’s Day Edition. Great wall ...   \n",
       "2      Doors of Dublin. The Dingus-the stuff that dre...   \n",
       "3      Good tight set up. Love the blue. Art. Flag. W...   \n",
       "4      Dark. Will never escape the stank. Wrong about...   \n",
       "...                                                  ...   \n",
       "12614  The blue works. Not just books a plus. Good us...   \n",
       "12615  This is the “I’ll just put on sweatpants” of S...   \n",
       "12616  All books. Too dark. Little in way of personal...   \n",
       "12617  Books always a must but a little too obvious f...   \n",
       "12618  She’s not even trying. 2/10.  https://t.co/MrA...   \n",
       "\n",
       "                                            tags_removed  \\\n",
       "0      [new, view, love, art, presentation, perspecti...   \n",
       "1      [st, paddy’s, day, edition, great, wall, color...   \n",
       "2      [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3      [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4      [dark, never, escape, stank, wrong, about, eve...   \n",
       "...                                                  ...   \n",
       "12614  [blue, works, not, just, books, plus, good, us...   \n",
       "12615      [“i’ll, just, put, sweatpants”, skype, rooms]   \n",
       "12616   [books, too, dark, little, way, personal, style]   \n",
       "12617  [books, always, must, little, too, obvious, su...   \n",
       "12618                         [she’s, not, even, trying]   \n",
       "\n",
       "                                                 stemmed  \\\n",
       "0      [new, view, love, art, present, perspect, ligh...   \n",
       "1      [st, paddy’s, dai, edit, great, wall, color, m...   \n",
       "2      [door, dublin, dingus-the, stuff, dream, made,...   \n",
       "3      [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4      [dark, never, escap, stank, wrong, about, ever...   \n",
       "...                                                  ...   \n",
       "12614  [blue, work, not, just, book, plu, good, us, s...   \n",
       "12615       [“i’ll, just, put, sweatpants”, skype, room]   \n",
       "12616       [book, too, dark, littl, wai, person, style]   \n",
       "12617  [book, alwai, must, littl, too, obviou, such, ...   \n",
       "12618                            [she’s, not, even, try]   \n",
       "\n",
       "                                                 default  \n",
       "0      [new, view, love, the, art, presentation, pers...  \n",
       "1      [room, rater, st, paddy’s, day, edition, great...  \n",
       "2      [doors, of, dublin, the, dingus-the, stuff, th...  \n",
       "3      [good, tight, set, up, love, the, blue, art, f...  \n",
       "4      [dark, will, never, escape, the, stank, wrong,...  \n",
       "...                                                  ...  \n",
       "12614  [the, blue, works, not, just, books, a, plus, ...  \n",
       "12615  [this, is, the, “i’ll, just, put, on, sweatpan...  \n",
       "12616  [all, books, too, dark, little, in, way, of, p...  \n",
       "12617  [books, always, a, must, but, a, little, too, ...  \n",
       "12618                       [she’s, not, even, trying, ]  \n",
       "\n",
       "[12619 rows x 6 columns]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "overhead-guide",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new',\n",
       " 'view',\n",
       " 'love',\n",
       " 'the',\n",
       " 'art',\n",
       " 'presentation',\n",
       " 'perspective',\n",
       " 'light/lighting',\n",
       " 'widen',\n",
       " 'shot',\n",
       " 'slightly',\n",
       " '@davidjollyfl']"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.iloc[0][\"default\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-footage",
   "metadata": {},
   "source": [
    "# **Utilizing LDA Topic Modeling**\n",
    "\n",
    "**Only LDA Topic Model:** \n",
    "   1. Fits data into LDA to get topic models, topic model is trained\n",
    "   2. Topic model is applied back to training reviews + each review gets topic\n",
    "      distribution/probabilities\n",
    "   3. Topic probabilities are used as features for rating prediction + fitted\n",
    "      into linear regression model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medium-heather",
   "metadata": {},
   "source": [
    "## Fitting Only LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "assisted-pursuit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora import Dictionary\n",
    "# preprocessed has columns: cleaned, tags_removed, stemmed, default"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-statement",
   "metadata": {},
   "source": [
    "### Creating a Bag of Words for Cleaned + Stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "unnecessary-format",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bag of words\n",
    "\n",
    "# cleaned column\n",
    "token_dict_cleaned = Dictionary(preprocessed.cleaned)\n",
    "bag_of_words_cleaned = [token_dict_cleaned.doc2bow(tweet) for tweet in preprocessed['cleaned']]\n",
    "\n",
    "# stemmed\n",
    "token_dict_stemmed = Dictionary(preprocessed.stemmed)\n",
    "bag_of_words_stemmed = [token_dict_stemmed.doc2bow(tweet) for tweet in preprocessed['stemmed']]\n",
    "\n",
    "# default\n",
    "token_dict_default = Dictionary(preprocessed.default)\n",
    "bag_of_words_default = [token_dict_default.doc2bow(tweet) for tweet in preprocessed['default']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-editor",
   "metadata": {},
   "source": [
    "### Fitting Cleaned + Stemmed data into LDA model with 5 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "temporal-briefing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.021*\"of\" + 0.014*\"books\" + 0.013*\"nice\" + 0.013*\"camera\" + 0.013*\"raise\" + 0.010*\"a\" + 0.010*\"and\" + 0.009*\"book\" + 0.009*\"for\" + 0.009*\"up\"'),\n",
       " (1,\n",
       "  '0.057*\"the\" + 0.034*\"art\" + 0.031*\"love\" + 0.022*\"and\" + 0.020*\"a\" + 0.016*\"add\" + 0.016*\"plant\" + 0.015*\"on\" + 0.014*\"light\" + 0.013*\"to\"'),\n",
       " (2,\n",
       "  '0.037*\"the\" + 0.033*\"a\" + 0.021*\"is\" + 0.019*\"we\" + 0.015*\"room\" + 0.013*\"for\" + 0.012*\"in\" + 0.012*\"\" + 0.011*\"to\" + 0.011*\"and\"'),\n",
       " (3,\n",
       "  '0.034*\"a\" + 0.029*\"room\" + 0.024*\"the\" + 0.022*\"\" + 0.018*\"to\" + 0.017*\"and\" + 0.017*\"is\" + 0.016*\"this\" + 0.016*\"in\" + 0.015*\"rater\"'),\n",
       " (4,\n",
       "  '0.053*\"well\" + 0.026*\"art\" + 0.020*\"lit\" + 0.017*\"good\" + 0.014*\"plant\" + 0.012*\"up\" + 0.011*\"a\" + 0.011*\"books\" + 0.010*\"set\" + 0.010*\"nice\"')]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into LDA (5 TOPICS)\n",
    "\n",
    "# cleaned\n",
    "lda_5_cleaned = LdaModel(bag_of_words_cleaned, \n",
    "                    num_topics = 5,\n",
    "                    id2word = token_dict_cleaned,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "#print(lda_5_cleaned.show_topics())\n",
    "\n",
    "# stemmed\n",
    "lda_5_stemmed = LdaModel(bag_of_words_stemmed, \n",
    "                    num_topics = 5,\n",
    "                    id2word = token_dict_stemmed,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_5_stemmed.show_topics()\n",
    "\n",
    "# default\n",
    "lda_5_default = LdaModel(bag_of_words_default, \n",
    "                    num_topics = 5,\n",
    "                    id2word = token_dict_default,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_5_default.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-independence",
   "metadata": {},
   "source": [
    "### Fitting Cleaned + Stemmed data into LDA model with 10 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "appropriate-option",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.087*\"camera\" + 0.073*\"raise\" + 0.020*\"lower\" + 0.016*\"nice\" + 0.013*\"slightly\" + 0.012*\"bigger\" + 0.012*\"beams\" + 0.012*\"reframe\" + 0.011*\"move\" + 0.010*\"height\"'),\n",
       " (1,\n",
       "  '0.069*\"the\" + 0.041*\"art\" + 0.040*\"love\" + 0.023*\"great\" + 0.019*\"plant\" + 0.017*\"lighting\" + 0.017*\"depth\" + 0.016*\"good\" + 0.016*\"for\" + 0.013*\"flowers\"'),\n",
       " (2,\n",
       "  '0.054*\"room\" + 0.040*\"a\" + 0.031*\"rater\" + 0.023*\"\" + 0.020*\"skype\" + 0.020*\"and\" + 0.020*\"for\" + 0.018*\"the\" + 0.017*\"rooms\" + 0.017*\"to\"'),\n",
       " (3,\n",
       "  '0.056*\"a\" + 0.029*\"is\" + 0.024*\"but\" + 0.024*\"to\" + 0.023*\"you\" + 0.020*\"this\" + 0.019*\"\" + 0.019*\"we\" + 0.019*\"not\" + 0.018*\"the\"'),\n",
       " (4,\n",
       "  '0.044*\"\" + 0.017*\"music\" + 0.012*\"start\" + 0.012*\"personal\" + 0.012*\"guy\" + 0.011*\"texture\" + 0.009*\"expect\" + 0.009*\"home\" + 0.008*\"perfectly\" + 0.008*\"hip\"'),\n",
       " (5,\n",
       "  '0.045*\"art\" + 0.034*\"a\" + 0.029*\"well\" + 0.026*\"add\" + 0.025*\"the\" + 0.022*\"plant\" + 0.020*\"good\" + 0.018*\"on\" + 0.018*\"love\" + 0.018*\"nice\"'),\n",
       " (6,\n",
       "  '0.031*\"doors\" + 0.021*\"deduction\" + 0.019*\"french\" + 0.012*\"stunning\" + 0.010*\"chic\" + 0.010*\"coded\" + 0.010*\"half\" + 0.009*\"point\" + 0.009*\"9\" + 0.009*\"hope\"'),\n",
       " (7,\n",
       "  '0.083*\"the\" + 0.039*\"and\" + 0.025*\"love\" + 0.021*\"to\" + 0.021*\"we\" + 0.020*\"a\" + 0.017*\"of\" + 0.016*\"is\" + 0.016*\"art\" + 0.014*\"more\"'),\n",
       " (8,\n",
       "  '0.035*\"up\" + 0.023*\"set\" + 0.023*\"books\" + 0.018*\"good\" + 0.017*\"of\" + 0.012*\"and\" + 0.011*\"book\" + 0.011*\"a\" + 0.009*\"corner\" + 0.009*\"horizontal\"'),\n",
       " (9,\n",
       "  '0.042*\"the\" + 0.032*\"of\" + 0.031*\"in\" + 0.028*\"room\" + 0.021*\"this\" + 0.020*\"hostage\" + 0.018*\"is\" + 0.017*\"video\" + 0.016*\"a\" + 0.014*\"with\"')]"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into LDA (10 TOPICS)\n",
    "\n",
    "# cleaned\n",
    "lda_10_cleaned = LdaModel(bag_of_words_cleaned, \n",
    "                    num_topics = 10,\n",
    "                    id2word = token_dict_cleaned,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "#print(lda_5_cleaned.show_topics())\n",
    "\n",
    "# stemmed\n",
    "lda_10_stemmed = LdaModel(bag_of_words_stemmed, \n",
    "                    num_topics = 10,\n",
    "                    id2word = token_dict_stemmed,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_10_stemmed.show_topics()\n",
    "\n",
    "# default\n",
    "lda_10_default = LdaModel(bag_of_words_default, \n",
    "                    num_topics = 10,\n",
    "                    id2word = token_dict_default,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_10_default.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attractive-constitutional",
   "metadata": {},
   "source": [
    "### Fitting Cleaned + Stemmed data into LDA model with 20 topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "eastern-billy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(10,\n",
       "  '0.101*\"love\" + 0.094*\"art\" + 0.076*\"great\" + 0.036*\"depth\" + 0.034*\"lighting\" + 0.021*\"plant\" + 0.021*\"flowers\" + 0.020*\"lamp\" + 0.017*\"chair\" + 0.015*\"blue\"'),\n",
       " (14,\n",
       "  '0.054*\"see\" + 0.031*\"so\" + 0.029*\"can\" + 0.028*\"space\" + 0.026*\"like\" + 0.023*\"more\" + 0.018*\"make\" + 0.018*\"would\" + 0.017*\"art\" + 0.017*\"fix\"'),\n",
       " (2,\n",
       "  '0.090*\"it’s\" + 0.038*\"always\" + 0.035*\"bad\" + 0.032*\"succulent\" + 0.026*\"way\" + 0.024*\"still\" + 0.021*\"top\" + 0.020*\"being\" + 0.016*\"dog\" + 0.015*\"position\"'),\n",
       " (16,\n",
       "  '0.080*\"out\" + 0.043*\"not\" + 0.029*\"ok\" + 0.029*\"crop\" + 0.026*\"he’s\" + 0.023*\"that’s\" + 0.022*\"ceiling\" + 0.018*\"doesn’t\" + 0.017*\"music\" + 0.017*\"sure\"'),\n",
       " (7,\n",
       "  '0.049*\"lovely\" + 0.030*\"than\" + 0.023*\"elegant\" + 0.021*\"cabinet\" + 0.019*\"open\" + 0.019*\"collection\" + 0.019*\"yellow\" + 0.019*\"nailed\" + 0.018*\"start\" + 0.017*\"update\"'),\n",
       " (3,\n",
       "  '0.025*\"there’s\" + 0.024*\"1\" + 0.021*\"we’ll\" + 0.020*\"we’ve\" + 0.017*\"@jheil\" + 0.016*\"seen\" + 0.015*\"yet\" + 0.015*\"someone\" + 0.014*\"help\" + 0.014*\"best\"'),\n",
       " (12,\n",
       "  '0.068*\"one\" + 0.057*\"just\" + 0.040*\"cool\" + 0.020*\"canada\" + 0.018*\"own\" + 0.018*\"book\" + 0.016*\"@brittlestar\" + 0.016*\"bigger\" + 0.015*\"almost\" + 0.015*\"keep\"'),\n",
       " (13,\n",
       "  '0.041*\"historic\" + 0.029*\"rooms\" + 0.026*\"got\" + 0.023*\"background\" + 0.023*\"photos\" + 0.022*\"lines\" + 0.022*\"change\" + 0.018*\"does\" + 0.015*\"these\" + 0.015*\"behind\"'),\n",
       " (9,\n",
       "  '0.066*\"books\" + 0.032*\"no\" + 0.027*\"could\" + 0.026*\"horizontal\" + 0.023*\"better\" + 0.022*\"real\" + 0.022*\"deduction\" + 0.021*\"even\" + 0.020*\"point\" + 0.017*\"oh\"'),\n",
       " (17,\n",
       "  '0.108*\"well\" + 0.094*\"up\" + 0.057*\"set\" + 0.047*\"nice\" + 0.046*\"good\" + 0.041*\"lit\" + 0.032*\"book\" + 0.024*\"books\" + 0.023*\"shelf\" + 0.016*\"shelves\"')]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting data into LDA (20 TOPICS)\n",
    "lda_20 = LdaModel(bag_of_words_cleaned, \n",
    "                    num_topics = 20,\n",
    "                    id2word = token_dict,\n",
    "                    random_state = 1, \n",
    "                    passes = 10)\n",
    "\n",
    "lda_20.show_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-powell",
   "metadata": {},
   "source": [
    "### For each cleaned + stemmed data, apply both 5 and 10 topic models to each tweet, output 5 topic and 10 topic probabilities for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "diverse-ecology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.010069676,\n",
       " 0.6693375,\n",
       " 0.010069676,\n",
       " 0.25009936,\n",
       " 0.010071219,\n",
       " 0.010070459,\n",
       " 0.010069675,\n",
       " 0.0100699365,\n",
       " 0.010070947,\n",
       " 0.010071544]"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_topic_probabilities_cleaned(tokens, num_topics):\n",
    "    bow = token_dict_cleaned.doc2bow(tokens)\n",
    "    if num_topics == 10: \n",
    "        topic_probs = lda_10_cleaned.get_document_topics(bow)\n",
    "    if num_topics == 5:\n",
    "        topic_probs = lda_5_cleaned.get_document_topics(bow)\n",
    "        \n",
    "    probs = []\n",
    "        \n",
    "    probs_dict = dict(topic_probs)\n",
    "        \n",
    "    for i in range (0, num_topics):\n",
    "        if i not in probs_dict: \n",
    "            probs.append(0)\n",
    "        else: \n",
    "            probs.append(probs_dict[i])\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def extract_topic_probabilities_stemmed(tokens, num_topics):\n",
    "    bow = token_dict_stemmed.doc2bow(tokens)\n",
    "    if num_topics == 10: \n",
    "        topic_probs = lda_10_stemmed.get_document_topics(bow)\n",
    "    if num_topics == 5:\n",
    "        topic_probs = lda_5_stemmed.get_document_topics(bow)\n",
    "        \n",
    "    probs = []\n",
    "        \n",
    "    probs_dict = dict(topic_probs)\n",
    "        \n",
    "    for i in range (0, num_topics):\n",
    "        if i not in probs_dict: \n",
    "            probs.append(0)\n",
    "        else: \n",
    "            probs.append(probs_dict[i])\n",
    "    \n",
    "    return probs\n",
    "\n",
    "def extract_topic_probabilities_default(tokens, num_topics):\n",
    "    bow = token_dict_default.doc2bow(tokens)\n",
    "    if num_topics == 10: \n",
    "        topic_probs = lda_10_default.get_document_topics(bow)\n",
    "    if num_topics == 5:\n",
    "        topic_probs = lda_5_default.get_document_topics(bow)\n",
    "        \n",
    "    probs = []\n",
    "        \n",
    "    probs_dict = dict(topic_probs)\n",
    "        \n",
    "    for i in range (0, num_topics):\n",
    "        if i not in probs_dict: \n",
    "            probs.append(0)\n",
    "        else: \n",
    "            probs.append(probs_dict[i])\n",
    "    \n",
    "    return probs\n",
    "   \n",
    "extract_topic_probabilities_cleaned(processed, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-respect",
   "metadata": {},
   "source": [
    "### Adding topic probabilities as columns to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "brutal-unknown",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "preprocessed['lda_10_probs_cleaned'] = preprocessed['cleaned'].apply(lambda x : extract_topic_probabilities_cleaned(x, 10))\n",
    "preprocessed['lda_5_probs_cleaned'] = preprocessed['cleaned'].apply(lambda x : extract_topic_probabilities_cleaned(x, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "mexican-italic",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['lda_10_probs_stemmed'] = preprocessed['stemmed'].apply(lambda x : extract_topic_probabilities_stemmed(x, 10))\n",
    "preprocessed['lda_5_probs_stemmed'] = preprocessed['stemmed'].apply(lambda x : extract_topic_probabilities_stemmed(x, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "removed-guatemala",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed['lda_10_probs_default'] = preprocessed['default'].apply(lambda x : extract_topic_probabilities_default(x, 10))\n",
    "preprocessed['lda_5_probs_default'] = preprocessed['default'].apply(lambda x : extract_topic_probabilities_default(x, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "trying-integration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>cleaned</th>\n",
       "      <th>tweet</th>\n",
       "      <th>tags_removed</th>\n",
       "      <th>stemmed</th>\n",
       "      <th>default</th>\n",
       "      <th>lda_10_probs_cleaned</th>\n",
       "      <th>lda_5_probs_cleaned</th>\n",
       "      <th>lda_10_probs_stemmed</th>\n",
       "      <th>lda_5_probs_stemmed</th>\n",
       "      <th>lda_10_probs_default</th>\n",
       "      <th>lda_5_probs_default</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>New view. Love the art. Presentation. Perspect...</td>\n",
       "      <td>[new, view, love, art, presentation, perspecti...</td>\n",
       "      <td>[new, view, love, art, present, perspect, ligh...</td>\n",
       "      <td>[new, view, love, the, art, presentation, pers...</td>\n",
       "      <td>[0, 0.68522346, 0, 0.2477858, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.11110278, 0.8379478, 0.016782334, 0.0173642...</td>\n",
       "      <td>[0, 0, 0, 0.14467387, 0, 0, 0, 0, 0.7753759, 0]</td>\n",
       "      <td>[0.018320857, 0.018461967, 0.018315086, 0.9264...</td>\n",
       "      <td>[0, 0.54525924, 0, 0, 0, 0.31564614, 0.0849685...</td>\n",
       "      <td>[0.015535021, 0.9377828, 0.015555755, 0.015524...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>Room Rater St Paddy’s Day Edition. Great wall ...</td>\n",
       "      <td>[st, paddy’s, day, edition, great, wall, color...</td>\n",
       "      <td>[st, paddy’s, dai, edit, great, wall, color, m...</td>\n",
       "      <td>[room, rater, st, paddy’s, day, edition, great...</td>\n",
       "      <td>[0, 0.11533039, 0, 0.5768728, 0, 0, 0, 0.26115...</td>\n",
       "      <td>[0.013347856, 0.31579176, 0.012890519, 0.57554...</td>\n",
       "      <td>[0, 0.31566647, 0, 0, 0, 0, 0, 0, 0.6309494, 0]</td>\n",
       "      <td>[0.013394717, 0.3650373, 0.013496481, 0.594382...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0.5381587, 0, 0, 0, 0.4173673]</td>\n",
       "      <td>[0.010641494, 0.23810284, 0.010682813, 0.33756...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>Doors of Dublin. The Dingus-the stuff that dre...</td>\n",
       "      <td>[doors, dublin, dingus-the, stuff, dreams, mad...</td>\n",
       "      <td>[door, dublin, dingus-the, stuff, dream, made,...</td>\n",
       "      <td>[doors, of, dublin, the, dingus-the, stuff, th...</td>\n",
       "      <td>[0.19798075, 0.22859469, 0, 0, 0, 0, 0.1222236...</td>\n",
       "      <td>[0.75999737, 0.19633839, 0.014439249, 0.014682...</td>\n",
       "      <td>[0, 0.057735033, 0, 0.54084694, 0, 0.1300493, ...</td>\n",
       "      <td>[0.10219681, 0.19941911, 0.014534805, 0.584886...</td>\n",
       "      <td>[0, 0, 0.10568784, 0, 0, 0, 0.060056217, 0.797...</td>\n",
       "      <td>[0.6363183, 0.33111113, 0.0109798545, 0.010921...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>Good tight set up. Love the blue. Art. Flag. W...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, blue, art, flag, ...</td>\n",
       "      <td>[good, tight, set, up, love, the, blue, art, f...</td>\n",
       "      <td>[0, 0, 0, 0.6056093, 0.3326137, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[0.015479418, 0.015946027, 0.015447189, 0.9374...</td>\n",
       "      <td>[0, 0, 0, 0.4312952, 0, 0, 0, 0, 0.50202626, 0]</td>\n",
       "      <td>[0.016847817, 0.01667914, 0.016725928, 0.93293...</td>\n",
       "      <td>[0, 0.13276435, 0, 0, 0, 0.738581, 0.07868052,...</td>\n",
       "      <td>[0.013553711, 0.94561774, 0.013505388, 0.01350...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>Dark. Will never escape the stank. Wrong about...</td>\n",
       "      <td>[dark, never, escape, stank, wrong, about, eve...</td>\n",
       "      <td>[dark, never, escap, stank, wrong, about, ever...</td>\n",
       "      <td>[dark, will, never, escape, the, stank, wrong,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0.39992622, 0.3857506, 0.15...</td>\n",
       "      <td>[0.018459152, 0.018509388, 0.7792248, 0.018318...</td>\n",
       "      <td>[0.01018724, 0.010193234, 0.2553968, 0.3359198...</td>\n",
       "      <td>[0.38353115, 0.5552803, 0.020458056, 0.0202665...</td>\n",
       "      <td>[0, 0, 0.29659498, 0.57335514, 0, 0, 0.0801155...</td>\n",
       "      <td>[0.012659157, 0.012793749, 0.4296478, 0.53226,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            cleaned  \\\n",
       "0       9  [new, view, love, art, presentation, perspecti...   \n",
       "1       9  [st, paddy’s, day, edition, great, wall, color...   \n",
       "2      10  [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3       9  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4       0  [dark, never, escape, stank, wrong, about, eve...   \n",
       "\n",
       "                                               tweet  \\\n",
       "0  New view. Love the art. Presentation. Perspect...   \n",
       "1  Room Rater St Paddy’s Day Edition. Great wall ...   \n",
       "2  Doors of Dublin. The Dingus-the stuff that dre...   \n",
       "3  Good tight set up. Love the blue. Art. Flag. W...   \n",
       "4  Dark. Will never escape the stank. Wrong about...   \n",
       "\n",
       "                                        tags_removed  \\\n",
       "0  [new, view, love, art, presentation, perspecti...   \n",
       "1  [st, paddy’s, day, edition, great, wall, color...   \n",
       "2  [doors, dublin, dingus-the, stuff, dreams, mad...   \n",
       "3  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4  [dark, never, escape, stank, wrong, about, eve...   \n",
       "\n",
       "                                             stemmed  \\\n",
       "0  [new, view, love, art, present, perspect, ligh...   \n",
       "1  [st, paddy’s, dai, edit, great, wall, color, m...   \n",
       "2  [door, dublin, dingus-the, stuff, dream, made,...   \n",
       "3  [good, tight, set, up, love, blue, art, flag, ...   \n",
       "4  [dark, never, escap, stank, wrong, about, ever...   \n",
       "\n",
       "                                             default  \\\n",
       "0  [new, view, love, the, art, presentation, pers...   \n",
       "1  [room, rater, st, paddy’s, day, edition, great...   \n",
       "2  [doors, of, dublin, the, dingus-the, stuff, th...   \n",
       "3  [good, tight, set, up, love, the, blue, art, f...   \n",
       "4  [dark, will, never, escape, the, stank, wrong,...   \n",
       "\n",
       "                                lda_10_probs_cleaned  \\\n",
       "0    [0, 0.68522346, 0, 0.2477858, 0, 0, 0, 0, 0, 0]   \n",
       "1  [0, 0.11533039, 0, 0.5768728, 0, 0, 0, 0.26115...   \n",
       "2  [0.19798075, 0.22859469, 0, 0, 0, 0, 0.1222236...   \n",
       "3     [0, 0, 0, 0.6056093, 0.3326137, 0, 0, 0, 0, 0]   \n",
       "4  [0, 0, 0, 0, 0, 0, 0.39992622, 0.3857506, 0.15...   \n",
       "\n",
       "                                 lda_5_probs_cleaned  \\\n",
       "0  [0.11110278, 0.8379478, 0.016782334, 0.0173642...   \n",
       "1  [0.013347856, 0.31579176, 0.012890519, 0.57554...   \n",
       "2  [0.75999737, 0.19633839, 0.014439249, 0.014682...   \n",
       "3  [0.015479418, 0.015946027, 0.015447189, 0.9374...   \n",
       "4  [0.018459152, 0.018509388, 0.7792248, 0.018318...   \n",
       "\n",
       "                                lda_10_probs_stemmed  \\\n",
       "0    [0, 0, 0, 0.14467387, 0, 0, 0, 0, 0.7753759, 0]   \n",
       "1    [0, 0.31566647, 0, 0, 0, 0, 0, 0, 0.6309494, 0]   \n",
       "2  [0, 0.057735033, 0, 0.54084694, 0, 0.1300493, ...   \n",
       "3    [0, 0, 0, 0.4312952, 0, 0, 0, 0, 0.50202626, 0]   \n",
       "4  [0.01018724, 0.010193234, 0.2553968, 0.3359198...   \n",
       "\n",
       "                                 lda_5_probs_stemmed  \\\n",
       "0  [0.018320857, 0.018461967, 0.018315086, 0.9264...   \n",
       "1  [0.013394717, 0.3650373, 0.013496481, 0.594382...   \n",
       "2  [0.10219681, 0.19941911, 0.014534805, 0.584886...   \n",
       "3  [0.016847817, 0.01667914, 0.016725928, 0.93293...   \n",
       "4  [0.38353115, 0.5552803, 0.020458056, 0.0202665...   \n",
       "\n",
       "                                lda_10_probs_default  \\\n",
       "0  [0, 0.54525924, 0, 0, 0, 0.31564614, 0.0849685...   \n",
       "1     [0, 0, 0, 0, 0, 0.5381587, 0, 0, 0, 0.4173673]   \n",
       "2  [0, 0, 0.10568784, 0, 0, 0, 0.060056217, 0.797...   \n",
       "3  [0, 0.13276435, 0, 0, 0, 0.738581, 0.07868052,...   \n",
       "4  [0, 0, 0.29659498, 0.57335514, 0, 0, 0.0801155...   \n",
       "\n",
       "                                 lda_5_probs_default  \n",
       "0  [0.015535021, 0.9377828, 0.015555755, 0.015524...  \n",
       "1  [0.010641494, 0.23810284, 0.010682813, 0.33756...  \n",
       "2  [0.6363183, 0.33111113, 0.0109798545, 0.010921...  \n",
       "3  [0.013553711, 0.94561774, 0.013505388, 0.01350...  \n",
       "4  [0.012659157, 0.012793749, 0.4296478, 0.53226,...  "
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "military-twins",
   "metadata": {},
   "source": [
    "### Creating dataframe for each number of topics and preprocessing pair, each dataframe has rating (y) and columns for each topic probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "neural-links",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/envs/nvfel/lib/python3.7/site-packages/pandas/core/frame.py:3188: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>0.015535</td>\n",
       "      <td>0.937783</td>\n",
       "      <td>0.015556</td>\n",
       "      <td>0.015524</td>\n",
       "      <td>0.015602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>0.010641</td>\n",
       "      <td>0.238103</td>\n",
       "      <td>0.010683</td>\n",
       "      <td>0.337561</td>\n",
       "      <td>0.403012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>0.636318</td>\n",
       "      <td>0.331111</td>\n",
       "      <td>0.010980</td>\n",
       "      <td>0.010922</td>\n",
       "      <td>0.010669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0.013554</td>\n",
       "      <td>0.945618</td>\n",
       "      <td>0.013505</td>\n",
       "      <td>0.013503</td>\n",
       "      <td>0.013820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.012659</td>\n",
       "      <td>0.012794</td>\n",
       "      <td>0.429648</td>\n",
       "      <td>0.532260</td>\n",
       "      <td>0.012639</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating         0         1         2         3         4\n",
       "0       9  0.015535  0.937783  0.015556  0.015524  0.015602\n",
       "1       9  0.010641  0.238103  0.010683  0.337561  0.403012\n",
       "2      10  0.636318  0.331111  0.010980  0.010922  0.010669\n",
       "3       9  0.013554  0.945618  0.013505  0.013503  0.013820\n",
       "4       0  0.012659  0.012794  0.429648  0.532260  0.012639"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 10 TOPICS: CLEANED\n",
    "lda_features_df_10_cleaned = preprocessed[['rating', 'lda_10_probs_cleaned']]\n",
    "lda_features_df_10_cleaned[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']] = pd.DataFrame(lda_features_df_10_cleaned.lda_10_probs_cleaned.tolist(), index=lda_features_df_10_cleaned.index)\n",
    "lda_features_df_10_cleaned = lda_features_df_10_cleaned[['rating', '0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "lda_features_df_10_cleaned\n",
    "\n",
    "# 10 TOPICS: STEMMED\n",
    "lda_features_df_10_stemmed = preprocessed[['rating', 'lda_10_probs_stemmed']]\n",
    "lda_features_df_10_stemmed[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']] = pd.DataFrame(lda_features_df_10_stemmed.lda_10_probs_stemmed.tolist(), index=lda_features_df_10_stemmed.index)\n",
    "lda_features_df_10_stemmed = lda_features_df_10_stemmed[['rating', '0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "lda_features_df_10_stemmed\n",
    "\n",
    "# 10 TOPICS: DEFAULT\n",
    "lda_features_df_10_default = preprocessed[['rating', 'lda_10_probs_default']]\n",
    "lda_features_df_10_default[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']] = pd.DataFrame(lda_features_df_10_default.lda_10_probs_default.tolist(), index=lda_features_df_10_default.index)\n",
    "lda_features_df_10_default = lda_features_df_10_default[['rating', '0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "lda_features_df_10_default\n",
    "\n",
    "# 5 TOPICS: CLEANED\n",
    "lda_features_df_5_cleaned = preprocessed[['rating', 'lda_5_probs_cleaned']]\n",
    "lda_features_df_5_cleaned[['0','1', '2', '3', '4']] = pd.DataFrame(lda_features_df_5_cleaned.lda_5_probs_cleaned.tolist(), index=lda_features_df_5_cleaned.index)\n",
    "lda_features_df_5_cleaned = lda_features_df_5_cleaned[['rating', '0','1', '2', '3', '4']]\n",
    "lda_features_df_5_cleaned\n",
    "\n",
    "# 5 TOPICS: STEMMED\n",
    "lda_features_df_5_stemmed = preprocessed[['rating', 'lda_5_probs_stemmed']]\n",
    "lda_features_df_5_stemmed[['0','1', '2', '3', '4']] = pd.DataFrame(lda_features_df_5_stemmed.lda_5_probs_stemmed.tolist(), index=lda_features_df_5_stemmed.index)\n",
    "lda_features_df_5_stemmed = lda_features_df_5_stemmed[['rating', '0','1', '2', '3', '4']]\n",
    "lda_features_df_5_stemmed\n",
    "\n",
    "# 5 TOPICS: DEFAULT\n",
    "lda_features_df_5_default = preprocessed[['rating', 'lda_5_probs_default']]\n",
    "lda_features_df_5_default[['0','1', '2', '3', '4']] = pd.DataFrame(lda_features_df_5_default.lda_5_probs_default.tolist(), index=lda_features_df_5_default.index)\n",
    "lda_features_df_5_default = lda_features_df_5_default[['rating', '0','1', '2', '3', '4']]\n",
    "lda_features_df_5_default.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brutal-heath",
   "metadata": {},
   "source": [
    "### Fitting Logistic Regression Model on above dataframes, features (X) = topic probabilities, output (y) = rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "ignored-black",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED SCORES\n",
      "10 cleaned training acc:  0.4409199048374306\n",
      "10 cleaned testing acc:  0.28385279098432825\n",
      "10 cleaned testing F1 score:  0.029420443359904833\n",
      "10 cleaned testing roc score:  0.5055003314023403\n",
      "STEMMED SCORES\n",
      "10 stemmed training acc:  0.44885011895321175\n",
      "10 stemmed testing acc:  0.29961260785349536\n",
      "10 stemmed testing F1 score:  0.03895414416761144\n",
      "10 stemmed testing roc score:  0.5124778559464753\n",
      "DEFAULT SCORES\n",
      "10 default training acc:  0.4020618556701031\n",
      "10 default testing acc:  0.2411516111991548\n",
      "10 default testing F1 score:  0.019901441157023664\n",
      "10 default testing roc score:  0.49485538335176105\n"
     ]
    }
   ],
   "source": [
    "# (10 TOPICS)\n",
    "\n",
    "# CLEANED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_10_cleaned))\n",
    "train = lda_features_df_10_cleaned[:train_size]\n",
    "test = lda_features_df_10_cleaned[train_size:]\n",
    "\n",
    "X_all = lda_features_df_10_cleaned[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_all = lda_features_df_10_cleaned['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_test = test['rating']\n",
    "\n",
    "labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "\n",
    "logm_10_cleaned = LogisticRegression()\n",
    "logm_10_cleaned.fit(X_train, y_train)\n",
    "\n",
    "print(\"CLEANED SCORES\")\n",
    "\n",
    "print(\"10 cleaned training acc: \", logm_10_cleaned.score(X_train, y_train))\n",
    "print(\"10 cleaned testing acc: \", logm_10_cleaned.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_10_cleaned.predict(X_test)\n",
    "print(\"10 cleaned testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"10 cleaned testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n",
    "# STEMMED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_10_stemmed))\n",
    "train = lda_features_df_10_stemmed[:train_size]\n",
    "test = lda_features_df_10_stemmed[train_size:]\n",
    "\n",
    "X_all = lda_features_df_10_stemmed[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_all = lda_features_df_10_stemmed['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_10_stemmed = LogisticRegression()\n",
    "logm_10_stemmed.fit(X_train, y_train)\n",
    "\n",
    "print(\"STEMMED SCORES\")\n",
    "\n",
    "print(\"10 stemmed training acc: \", logm_10_stemmed.score(X_train, y_train))\n",
    "print(\"10 stemmed testing acc: \", logm_10_stemmed.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_10_stemmed.predict(X_test)\n",
    "print(\"10 stemmed testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"10 stemmed testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n",
    "# DEFAULT\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_10_default))\n",
    "train = lda_features_df_10_default[:train_size]\n",
    "test = lda_features_df_10_default[train_size:]\n",
    "\n",
    "X_all = lda_features_df_10_default[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_all = lda_features_df_10_default['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4', '5', '6', '7', '8', '9']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_10_default = LogisticRegression()\n",
    "logm_10_default.fit(X_train, y_train)\n",
    "\n",
    "print(\"DEFAULT SCORES\")\n",
    "\n",
    "print(\"10 default training acc: \", logm_10_stemmed.score(X_train, y_train))\n",
    "print(\"10 default testing acc: \", logm_10_stemmed.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_10_stemmed.predict(X_test)\n",
    "print(\"10 default testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"10 default testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "round-album",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLEANED SCORES\n",
      "5 cleaned training score:  0.4274385408406027\n",
      "5 cleaned testing score:  0.2900158478605388\n",
      "5 cleaned testing F1 score:  0.030602484476379446\n",
      "5 cleaned testing roc score:  0.5072406203665009\n",
      "STEMMED SCORES\n",
      "5 stemmed training score:  0.43536875495638383\n",
      "5 stemmed testing score:  0.30198978693431944\n",
      "5 stemmed testing F1 score:  0.033609750498231734\n",
      "5 stemmed testing roc score:  0.5108904819148777\n",
      "STEMMED SCORES\n",
      "5 default training score:  0.4425059476605868\n",
      "5 default testing score:  0.29503433703116744\n",
      "5 default testing F1 score:  0.03548037647244931\n",
      "5 default testing roc score:  0.5087845086358085\n"
     ]
    }
   ],
   "source": [
    "# fitting logistic regression model on training data (5 TOPICS)\n",
    "\n",
    "# CLEANED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_5_cleaned))\n",
    "train = lda_features_df_5_cleaned[:train_size]\n",
    "test = lda_features_df_5_cleaned[train_size:]\n",
    "\n",
    "X_all = lda_features_df_5_cleaned[['0','1', '2', '3', '4']]\n",
    "y_all = lda_features_df_5_cleaned['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_5_cleaned = LogisticRegression()\n",
    "logm_5_cleaned.fit(X_train, y_train)\n",
    "\n",
    "print(\"CLEANED SCORES\")\n",
    "\n",
    "print(\"5 cleaned training score: \", logm_5_cleaned.score(X_train, y_train))\n",
    "print(\"5 cleaned testing score: \", logm_5_cleaned.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_5_cleaned.predict(X_test)\n",
    "print(\"5 cleaned testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"5 cleaned testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n",
    "# STEMMED\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_5_stemmed))\n",
    "train = lda_features_df_5_stemmed[:train_size]\n",
    "test = lda_features_df_5_stemmed[train_size:]\n",
    "\n",
    "X_all = lda_features_df_5_stemmed[['0','1', '2', '3', '4']]\n",
    "y_all = lda_features_df_5_stemmed['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_5_stemmed = LogisticRegression()\n",
    "logm_5_stemmed.fit(X_train, y_train)\n",
    "\n",
    "print(\"STEMMED SCORES\")\n",
    "\n",
    "print(\"5 stemmed training score: \", logm_5_stemmed.score(X_train, y_train))\n",
    "print(\"5 stemmed testing score: \", logm_5_stemmed.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_5_stemmed.predict(X_test)\n",
    "print(\"5 stemmed testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"5 stemmed testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))\n",
    "\n",
    "\n",
    "# DEFAULT\n",
    "\n",
    "train_size = int(0.1 * len(lda_features_df_5_default))\n",
    "train = lda_features_df_5_default[:train_size]\n",
    "test = lda_features_df_5_default[train_size:]\n",
    "\n",
    "X_all = lda_features_df_5_default[['0','1', '2', '3', '4']]\n",
    "y_all = lda_features_df_5_default['rating']\n",
    "\n",
    "X_train = train[['0','1', '2', '3', '4']]\n",
    "y_train = train['rating']\n",
    "\n",
    "X_test = test[['0','1', '2', '3', '4']]\n",
    "y_test = test['rating']\n",
    "\n",
    "logm_5_default = LogisticRegression()\n",
    "logm_5_default.fit(X_train, y_train)\n",
    "\n",
    "print(\"DEFAULT SCORES\")\n",
    "\n",
    "print(\"5 default training score: \", logm_5_default.score(X_train, y_train))\n",
    "print(\"5 default testing score: \", logm_5_default.score(X_test, y_test))\n",
    "\n",
    "y_pred = logm_5_default.predict(X_test)\n",
    "print(\"5 default testing F1 score: \", f1_score(y_test, y_pred, average = 'macro'))\n",
    "\n",
    "y_test_binarized = label_binarize(list(y_test), classes = labels)\n",
    "y_preds_binarized = label_binarize(list(y_pred), classes = labels)\n",
    "print(\"5 default testing roc score: \", roc_auc_score(y_test_binarized, y_preds_binarized, average = 'macro', multi_class = 'ovo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-special",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
